{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KrWDSBMjpNI0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense ,Dropout\n",
        "from keras.optimizers import Adam\n",
        "import time"
      ],
      "id": "KrWDSBMjpNI0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqeM7ehPjkdW"
      },
      "source": [
        "# remplacer les valeurs manquantes par **le moyenne de chaque state**"
      ],
      "id": "aqeM7ehPjkdW"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "AUwDbJkHjics"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df=pd.read_csv('Water_dataX_mean_state_hong_kong.csv')\n",
        "cols_to_drop=['water control zone','river','station','dates','sample no','wqi','wqi clf']\n",
        "# Diviser les données en caractéristiques et cibles\n",
        "# Split the dataset into input (X) and output (y) variables\n",
        "X =  df.drop(cols_to_drop, axis=1)\n",
        "y = df[\"wqi\"].values"
      ],
      "id": "AUwDbJkHjics"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oVp6i2fqj9dQ"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "id": "oVp6i2fqj9dQ"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JYQfYsstkAa6"
      },
      "outputs": [],
      "source": [
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "id": "JYQfYsstkAa6"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Dd9hSTzykDX-"
      },
      "outputs": [],
      "source": [
        "# Reshape the input features for use with CNN\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ],
      "id": "Dd9hSTzykDX-"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xofnfQmVkF6v"
      },
      "outputs": [],
      "source": [
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=1))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128, activation='relu'))\n",
        "model.add(Dense(units=1, activation='linear'))"
      ],
      "id": "xofnfQmVkF6v"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J5E0ASBUkKPC"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mape')"
      ],
      "id": "J5E0ASBUkKPC"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4NLfpuXkMtL",
        "outputId": "bb1a0f69-eed1-4e79-9d3e-f426b07211ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2861 - val_loss: 2.3962\n",
            "Epoch 502/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2978 - val_loss: 2.7397\n",
            "Epoch 503/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2693 - val_loss: 2.4130\n",
            "Epoch 504/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2211 - val_loss: 2.3999\n",
            "Epoch 505/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1968 - val_loss: 2.7412\n",
            "Epoch 506/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2957 - val_loss: 2.3809\n",
            "Epoch 507/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2271 - val_loss: 2.3792\n",
            "Epoch 508/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2365 - val_loss: 2.5311\n",
            "Epoch 509/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2960 - val_loss: 2.3870\n",
            "Epoch 510/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2220 - val_loss: 2.3815\n",
            "Epoch 511/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2523 - val_loss: 2.4269\n",
            "Epoch 512/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2318 - val_loss: 2.4418\n",
            "Epoch 513/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2298 - val_loss: 2.6857\n",
            "Epoch 514/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2558 - val_loss: 2.3793\n",
            "Epoch 515/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2760 - val_loss: 2.3690\n",
            "Epoch 516/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2524 - val_loss: 2.3641\n",
            "Epoch 517/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2772 - val_loss: 2.3715\n",
            "Epoch 518/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2402 - val_loss: 2.4053\n",
            "Epoch 519/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2703 - val_loss: 2.4372\n",
            "Epoch 520/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.3047 - val_loss: 2.3822\n",
            "Epoch 521/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1858 - val_loss: 2.3832\n",
            "Epoch 522/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2486 - val_loss: 2.3855\n",
            "Epoch 523/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2504 - val_loss: 2.5485\n",
            "Epoch 524/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2727 - val_loss: 2.4284\n",
            "Epoch 525/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2235 - val_loss: 2.3640\n",
            "Epoch 526/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1817 - val_loss: 2.3691\n",
            "Epoch 527/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2177 - val_loss: 2.4969\n",
            "Epoch 528/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.2948 - val_loss: 2.6317\n",
            "Epoch 529/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.3101 - val_loss: 2.3824\n",
            "Epoch 530/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2205 - val_loss: 2.5175\n",
            "Epoch 531/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2151 - val_loss: 2.3601\n",
            "Epoch 532/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2441 - val_loss: 2.8387\n",
            "Epoch 533/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2601 - val_loss: 2.3850\n",
            "Epoch 534/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2638 - val_loss: 2.4029\n",
            "Epoch 535/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2039 - val_loss: 2.3723\n",
            "Epoch 536/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2543 - val_loss: 2.4275\n",
            "Epoch 537/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.3055 - val_loss: 2.4639\n",
            "Epoch 538/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2505 - val_loss: 2.5902\n",
            "Epoch 539/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 2.2433 - val_loss: 2.4332\n",
            "Epoch 540/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2264 - val_loss: 2.4780\n",
            "Epoch 541/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2879 - val_loss: 2.3766\n",
            "Epoch 542/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2776 - val_loss: 2.5096\n",
            "Epoch 543/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2159 - val_loss: 2.4220\n",
            "Epoch 544/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2645 - val_loss: 2.3929\n",
            "Epoch 545/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1964 - val_loss: 2.3933\n",
            "Epoch 546/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2289 - val_loss: 2.3230\n",
            "Epoch 547/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1749 - val_loss: 2.4005\n",
            "Epoch 548/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2390 - val_loss: 2.4357\n",
            "Epoch 549/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2293 - val_loss: 2.5485\n",
            "Epoch 550/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2625 - val_loss: 2.5113\n",
            "Epoch 551/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.2119 - val_loss: 2.4383\n",
            "Epoch 552/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2007 - val_loss: 2.4740\n",
            "Epoch 553/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2322 - val_loss: 2.4780\n",
            "Epoch 554/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2802 - val_loss: 2.4590\n",
            "Epoch 555/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2618 - val_loss: 2.9033\n",
            "Epoch 556/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2228 - val_loss: 2.4304\n",
            "Epoch 557/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2323 - val_loss: 2.4761\n",
            "Epoch 558/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2474 - val_loss: 2.4065\n",
            "Epoch 559/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2302 - val_loss: 2.7735\n",
            "Epoch 560/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1989 - val_loss: 2.3591\n",
            "Epoch 561/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2814 - val_loss: 2.4128\n",
            "Epoch 562/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1930 - val_loss: 2.5086\n",
            "Epoch 563/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2062 - val_loss: 2.4549\n",
            "Epoch 564/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2091 - val_loss: 2.5310\n",
            "Epoch 565/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2393 - val_loss: 2.5193\n",
            "Epoch 566/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2090 - val_loss: 2.3689\n",
            "Epoch 567/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2553 - val_loss: 2.4216\n",
            "Epoch 568/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2127 - val_loss: 2.4190\n",
            "Epoch 569/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2417 - val_loss: 2.3438\n",
            "Epoch 570/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2829 - val_loss: 2.3998\n",
            "Epoch 571/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2426 - val_loss: 2.3969\n",
            "Epoch 572/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2384 - val_loss: 2.5086\n",
            "Epoch 573/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2923 - val_loss: 2.5474\n",
            "Epoch 574/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1998 - val_loss: 2.3838\n",
            "Epoch 575/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2149 - val_loss: 2.4311\n",
            "Epoch 576/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1890 - val_loss: 2.5241\n",
            "Epoch 577/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2416 - val_loss: 2.4146\n",
            "Epoch 578/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2245 - val_loss: 2.3877\n",
            "Epoch 579/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1818 - val_loss: 2.3857\n",
            "Epoch 580/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.2033 - val_loss: 2.5286\n",
            "Epoch 581/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2179 - val_loss: 2.4981\n",
            "Epoch 582/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1857 - val_loss: 2.4300\n",
            "Epoch 583/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1974 - val_loss: 2.4048\n",
            "Epoch 584/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2352 - val_loss: 2.5944\n",
            "Epoch 585/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2277 - val_loss: 2.4099\n",
            "Epoch 586/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1901 - val_loss: 2.3762\n",
            "Epoch 587/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2800 - val_loss: 2.6308\n",
            "Epoch 588/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2292 - val_loss: 2.3201\n",
            "Epoch 589/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1706 - val_loss: 2.3598\n",
            "Epoch 590/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2173 - val_loss: 2.4049\n",
            "Epoch 591/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2809 - val_loss: 2.4553\n",
            "Epoch 592/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1869 - val_loss: 2.4364\n",
            "Epoch 593/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1642 - val_loss: 2.3613\n",
            "Epoch 594/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2065 - val_loss: 2.4822\n",
            "Epoch 595/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2081 - val_loss: 2.3156\n",
            "Epoch 596/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2040 - val_loss: 2.5373\n",
            "Epoch 597/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2852 - val_loss: 2.4161\n",
            "Epoch 598/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2024 - val_loss: 2.3839\n",
            "Epoch 599/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1630 - val_loss: 2.4272\n",
            "Epoch 600/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2430 - val_loss: 2.3716\n",
            "Epoch 601/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2188 - val_loss: 2.3960\n",
            "Epoch 602/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2443 - val_loss: 2.5261\n",
            "Epoch 603/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1984 - val_loss: 2.3745\n",
            "Epoch 604/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.2051 - val_loss: 2.4088\n",
            "Epoch 605/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.2125 - val_loss: 2.5150\n",
            "Epoch 606/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1944 - val_loss: 2.5269\n",
            "Epoch 607/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2132 - val_loss: 2.4198\n",
            "Epoch 608/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1431 - val_loss: 2.4798\n",
            "Epoch 609/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1646 - val_loss: 2.4232\n",
            "Epoch 610/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2073 - val_loss: 2.3902\n",
            "Epoch 611/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1963 - val_loss: 2.4293\n",
            "Epoch 612/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1605 - val_loss: 2.4426\n",
            "Epoch 613/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1487 - val_loss: 2.3517\n",
            "Epoch 614/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1951 - val_loss: 2.3486\n",
            "Epoch 615/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1900 - val_loss: 2.6228\n",
            "Epoch 616/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1877 - val_loss: 2.3480\n",
            "Epoch 617/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2282 - val_loss: 2.5325\n",
            "Epoch 618/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2425 - val_loss: 2.4394\n",
            "Epoch 619/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1972 - val_loss: 2.3605\n",
            "Epoch 620/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2150 - val_loss: 2.6807\n",
            "Epoch 621/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1635 - val_loss: 2.3390\n",
            "Epoch 622/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2026 - val_loss: 2.7332\n",
            "Epoch 623/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2062 - val_loss: 2.3723\n",
            "Epoch 624/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2073 - val_loss: 2.5852\n",
            "Epoch 625/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2196 - val_loss: 2.5284\n",
            "Epoch 626/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1678 - val_loss: 2.3204\n",
            "Epoch 627/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1508 - val_loss: 2.3467\n",
            "Epoch 628/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1894 - val_loss: 2.4445\n",
            "Epoch 629/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1978 - val_loss: 2.3890\n",
            "Epoch 630/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.2111 - val_loss: 2.3986\n",
            "Epoch 631/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1688 - val_loss: 2.3760\n",
            "Epoch 632/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.3895 - val_loss: 3.0961\n",
            "Epoch 633/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2233 - val_loss: 2.3959\n",
            "Epoch 634/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.2259 - val_loss: 2.3768\n",
            "Epoch 635/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1501 - val_loss: 2.3227\n",
            "Epoch 636/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1538 - val_loss: 2.3447\n",
            "Epoch 637/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1704 - val_loss: 2.3878\n",
            "Epoch 638/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2042 - val_loss: 2.3252\n",
            "Epoch 639/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.2074 - val_loss: 2.3929\n",
            "Epoch 640/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1649 - val_loss: 2.3825\n",
            "Epoch 641/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2253 - val_loss: 2.3773\n",
            "Epoch 642/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1615 - val_loss: 2.3146\n",
            "Epoch 643/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2145 - val_loss: 2.7114\n",
            "Epoch 644/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.2169 - val_loss: 2.4337\n",
            "Epoch 645/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1734 - val_loss: 2.4249\n",
            "Epoch 646/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2133 - val_loss: 2.4633\n",
            "Epoch 647/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2143 - val_loss: 2.3856\n",
            "Epoch 648/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1909 - val_loss: 2.5733\n",
            "Epoch 649/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1496 - val_loss: 2.3491\n",
            "Epoch 650/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1895 - val_loss: 2.4243\n",
            "Epoch 651/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2334 - val_loss: 2.4331\n",
            "Epoch 652/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1561 - val_loss: 2.3205\n",
            "Epoch 653/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2070 - val_loss: 2.4430\n",
            "Epoch 654/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1356 - val_loss: 2.3619\n",
            "Epoch 655/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1943 - val_loss: 2.3209\n",
            "Epoch 656/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1645 - val_loss: 2.3860\n",
            "Epoch 657/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1521 - val_loss: 2.3594\n",
            "Epoch 658/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1226 - val_loss: 2.4121\n",
            "Epoch 659/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1735 - val_loss: 2.3155\n",
            "Epoch 660/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1627 - val_loss: 2.3582\n",
            "Epoch 661/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1934 - val_loss: 2.5104\n",
            "Epoch 662/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1805 - val_loss: 2.2944\n",
            "Epoch 663/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1504 - val_loss: 2.4458\n",
            "Epoch 664/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1722 - val_loss: 2.5886\n",
            "Epoch 665/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 2.2063 - val_loss: 2.3492\n",
            "Epoch 666/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1287 - val_loss: 2.3356\n",
            "Epoch 667/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1827 - val_loss: 2.3858\n",
            "Epoch 668/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.2296 - val_loss: 2.3679\n",
            "Epoch 669/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1328 - val_loss: 2.3689\n",
            "Epoch 670/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1797 - val_loss: 2.3494\n",
            "Epoch 671/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1749 - val_loss: 2.4573\n",
            "Epoch 672/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2399 - val_loss: 2.5128\n",
            "Epoch 673/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1555 - val_loss: 2.3756\n",
            "Epoch 674/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1567 - val_loss: 2.3279\n",
            "Epoch 675/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2290 - val_loss: 2.3633\n",
            "Epoch 676/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1989 - val_loss: 2.5218\n",
            "Epoch 677/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1646 - val_loss: 2.3319\n",
            "Epoch 678/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1394 - val_loss: 2.4375\n",
            "Epoch 679/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1846 - val_loss: 2.4523\n",
            "Epoch 680/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1844 - val_loss: 2.5106\n",
            "Epoch 681/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2465 - val_loss: 2.3465\n",
            "Epoch 682/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1289 - val_loss: 2.3738\n",
            "Epoch 683/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1517 - val_loss: 2.3669\n",
            "Epoch 684/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1489 - val_loss: 2.3299\n",
            "Epoch 685/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.0936 - val_loss: 2.4681\n",
            "Epoch 686/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1107 - val_loss: 2.3911\n",
            "Epoch 687/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1604 - val_loss: 2.3521\n",
            "Epoch 688/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1626 - val_loss: 2.4670\n",
            "Epoch 689/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.2166 - val_loss: 2.3586\n",
            "Epoch 690/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1781 - val_loss: 2.5190\n",
            "Epoch 691/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2060 - val_loss: 2.8993\n",
            "Epoch 692/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1680 - val_loss: 2.3086\n",
            "Epoch 693/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0875 - val_loss: 2.3629\n",
            "Epoch 694/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2823 - val_loss: 2.4655\n",
            "Epoch 695/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2006 - val_loss: 2.3186\n",
            "Epoch 696/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1583 - val_loss: 2.3291\n",
            "Epoch 697/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1292 - val_loss: 2.4501\n",
            "Epoch 698/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2090 - val_loss: 2.3062\n",
            "Epoch 699/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1518 - val_loss: 2.3658\n",
            "Epoch 700/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1587 - val_loss: 2.3725\n",
            "Epoch 701/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1228 - val_loss: 2.3198\n",
            "Epoch 702/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1451 - val_loss: 2.4631\n",
            "Epoch 703/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1486 - val_loss: 2.3717\n",
            "Epoch 704/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1246 - val_loss: 2.3407\n",
            "Epoch 705/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1398 - val_loss: 2.4035\n",
            "Epoch 706/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1858 - val_loss: 2.3635\n",
            "Epoch 707/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1028 - val_loss: 2.3057\n",
            "Epoch 708/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1613 - val_loss: 2.4421\n",
            "Epoch 709/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1081 - val_loss: 2.3770\n",
            "Epoch 710/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1468 - val_loss: 2.7919\n",
            "Epoch 711/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2149 - val_loss: 2.5283\n",
            "Epoch 712/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1384 - val_loss: 2.3897\n",
            "Epoch 713/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1346 - val_loss: 2.3699\n",
            "Epoch 714/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1646 - val_loss: 2.3233\n",
            "Epoch 715/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1244 - val_loss: 2.3327\n",
            "Epoch 716/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1970 - val_loss: 2.5003\n",
            "Epoch 717/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1751 - val_loss: 2.3889\n",
            "Epoch 718/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1708 - val_loss: 2.3700\n",
            "Epoch 719/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1776 - val_loss: 2.4624\n",
            "Epoch 720/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1685 - val_loss: 2.3613\n",
            "Epoch 721/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2143 - val_loss: 2.4349\n",
            "Epoch 722/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1110 - val_loss: 2.3406\n",
            "Epoch 723/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1329 - val_loss: 2.4555\n",
            "Epoch 724/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1570 - val_loss: 2.4944\n",
            "Epoch 725/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1281 - val_loss: 2.3494\n",
            "Epoch 726/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.1577 - val_loss: 2.4060\n",
            "Epoch 727/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 2.1003 - val_loss: 2.4342\n",
            "Epoch 728/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1664 - val_loss: 2.3001\n",
            "Epoch 729/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1440 - val_loss: 2.3974\n",
            "Epoch 730/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0995 - val_loss: 2.4259\n",
            "Epoch 731/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1189 - val_loss: 2.6464\n",
            "Epoch 732/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1382 - val_loss: 2.3179\n",
            "Epoch 733/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1974 - val_loss: 2.5466\n",
            "Epoch 734/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1279 - val_loss: 2.4955\n",
            "Epoch 735/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1367 - val_loss: 2.3917\n",
            "Epoch 736/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1420 - val_loss: 2.3689\n",
            "Epoch 737/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1832 - val_loss: 2.5946\n",
            "Epoch 738/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1439 - val_loss: 2.4103\n",
            "Epoch 739/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1480 - val_loss: 2.4274\n",
            "Epoch 740/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1431 - val_loss: 2.3457\n",
            "Epoch 741/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0886 - val_loss: 2.3063\n",
            "Epoch 742/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1398 - val_loss: 2.3692\n",
            "Epoch 743/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1299 - val_loss: 2.4892\n",
            "Epoch 744/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1163 - val_loss: 2.3913\n",
            "Epoch 745/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1714 - val_loss: 2.3482\n",
            "Epoch 746/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1696 - val_loss: 2.3437\n",
            "Epoch 747/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1715 - val_loss: 2.3609\n",
            "Epoch 748/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1279 - val_loss: 2.3546\n",
            "Epoch 749/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1543 - val_loss: 2.3555\n",
            "Epoch 750/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0958 - val_loss: 2.5087\n",
            "Epoch 751/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1858 - val_loss: 2.3269\n",
            "Epoch 752/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1431 - val_loss: 2.3828\n",
            "Epoch 753/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1145 - val_loss: 2.4212\n",
            "Epoch 754/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1135 - val_loss: 2.3913\n",
            "Epoch 755/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2171 - val_loss: 2.4708\n",
            "Epoch 756/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1644 - val_loss: 2.3402\n",
            "Epoch 757/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1103 - val_loss: 2.3040\n",
            "Epoch 758/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1446 - val_loss: 2.3697\n",
            "Epoch 759/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0860 - val_loss: 2.2990\n",
            "Epoch 760/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1109 - val_loss: 2.4099\n",
            "Epoch 761/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1868 - val_loss: 2.3454\n",
            "Epoch 762/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1437 - val_loss: 2.3646\n",
            "Epoch 763/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1346 - val_loss: 2.3167\n",
            "Epoch 764/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1354 - val_loss: 2.3441\n",
            "Epoch 765/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1133 - val_loss: 2.2804\n",
            "Epoch 766/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0907 - val_loss: 2.3402\n",
            "Epoch 767/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1152 - val_loss: 2.3332\n",
            "Epoch 768/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1346 - val_loss: 2.3936\n",
            "Epoch 769/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0765 - val_loss: 2.3826\n",
            "Epoch 770/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1393 - val_loss: 2.3187\n",
            "Epoch 771/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0915 - val_loss: 2.3611\n",
            "Epoch 772/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0709 - val_loss: 2.2990\n",
            "Epoch 773/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1004 - val_loss: 2.3641\n",
            "Epoch 774/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1430 - val_loss: 2.3510\n",
            "Epoch 775/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1402 - val_loss: 2.4728\n",
            "Epoch 776/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0964 - val_loss: 2.3491\n",
            "Epoch 777/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1414 - val_loss: 2.3343\n",
            "Epoch 778/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1435 - val_loss: 2.3647\n",
            "Epoch 779/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1726 - val_loss: 2.3731\n",
            "Epoch 780/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1983 - val_loss: 2.4251\n",
            "Epoch 781/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1674 - val_loss: 2.3829\n",
            "Epoch 782/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1996 - val_loss: 2.3683\n",
            "Epoch 783/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1362 - val_loss: 2.3186\n",
            "Epoch 784/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1149 - val_loss: 2.4286\n",
            "Epoch 785/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1247 - val_loss: 2.3174\n",
            "Epoch 786/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0932 - val_loss: 2.3941\n",
            "Epoch 787/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1186 - val_loss: 2.5815\n",
            "Epoch 788/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1124 - val_loss: 2.3220\n",
            "Epoch 789/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1014 - val_loss: 2.4108\n",
            "Epoch 790/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1105 - val_loss: 2.4305\n",
            "Epoch 791/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1450 - val_loss: 2.4255\n",
            "Epoch 792/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0949 - val_loss: 2.2722\n",
            "Epoch 793/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0685 - val_loss: 2.2588\n",
            "Epoch 794/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1362 - val_loss: 2.3145\n",
            "Epoch 795/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1357 - val_loss: 2.3467\n",
            "Epoch 796/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1205 - val_loss: 2.2687\n",
            "Epoch 797/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0922 - val_loss: 2.2818\n",
            "Epoch 798/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0885 - val_loss: 2.3614\n",
            "Epoch 799/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0672 - val_loss: 2.3616\n",
            "Epoch 800/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1022 - val_loss: 2.3301\n",
            "Epoch 801/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1282 - val_loss: 2.3201\n",
            "Epoch 802/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.2081 - val_loss: 2.4403\n",
            "Epoch 803/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1567 - val_loss: 2.5154\n",
            "Epoch 804/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0746 - val_loss: 2.3392\n",
            "Epoch 805/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0631 - val_loss: 2.2900\n",
            "Epoch 806/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0812 - val_loss: 2.3651\n",
            "Epoch 807/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0509 - val_loss: 2.3171\n",
            "Epoch 808/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1439 - val_loss: 2.4004\n",
            "Epoch 809/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0971 - val_loss: 2.3200\n",
            "Epoch 810/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0834 - val_loss: 2.3168\n",
            "Epoch 811/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0842 - val_loss: 2.3830\n",
            "Epoch 812/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1215 - val_loss: 2.3835\n",
            "Epoch 813/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1115 - val_loss: 2.2792\n",
            "Epoch 814/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.2861 - val_loss: 2.6068\n",
            "Epoch 815/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0596 - val_loss: 2.2784\n",
            "Epoch 816/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1560 - val_loss: 2.5658\n",
            "Epoch 817/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1173 - val_loss: 2.3442\n",
            "Epoch 818/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1128 - val_loss: 2.4041\n",
            "Epoch 819/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1009 - val_loss: 2.3281\n",
            "Epoch 820/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1227 - val_loss: 2.3000\n",
            "Epoch 821/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1093 - val_loss: 2.3495\n",
            "Epoch 822/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1354 - val_loss: 2.3063\n",
            "Epoch 823/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0649 - val_loss: 2.2918\n",
            "Epoch 824/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1513 - val_loss: 2.2886\n",
            "Epoch 825/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0313 - val_loss: 2.2901\n",
            "Epoch 826/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.1041 - val_loss: 2.3755\n",
            "Epoch 827/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0779 - val_loss: 2.3256\n",
            "Epoch 828/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0985 - val_loss: 2.6272\n",
            "Epoch 829/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0933 - val_loss: 2.2485\n",
            "Epoch 830/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0903 - val_loss: 2.3506\n",
            "Epoch 831/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0608 - val_loss: 2.3394\n",
            "Epoch 832/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0746 - val_loss: 2.3916\n",
            "Epoch 833/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1267 - val_loss: 2.4769\n",
            "Epoch 834/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1595 - val_loss: 2.3543\n",
            "Epoch 835/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1331 - val_loss: 2.2808\n",
            "Epoch 836/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0570 - val_loss: 2.2899\n",
            "Epoch 837/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1164 - val_loss: 2.3965\n",
            "Epoch 838/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0846 - val_loss: 2.4827\n",
            "Epoch 839/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0951 - val_loss: 2.5003\n",
            "Epoch 840/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0801 - val_loss: 2.5261\n",
            "Epoch 841/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1112 - val_loss: 2.3470\n",
            "Epoch 842/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1035 - val_loss: 2.3219\n",
            "Epoch 843/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0641 - val_loss: 2.2810\n",
            "Epoch 844/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0896 - val_loss: 2.3283\n",
            "Epoch 845/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0513 - val_loss: 2.2873\n",
            "Epoch 846/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0942 - val_loss: 2.5452\n",
            "Epoch 847/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1053 - val_loss: 2.3815\n",
            "Epoch 848/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1170 - val_loss: 2.3359\n",
            "Epoch 849/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1044 - val_loss: 2.3920\n",
            "Epoch 850/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1114 - val_loss: 2.4755\n",
            "Epoch 851/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0980 - val_loss: 2.3832\n",
            "Epoch 852/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1119 - val_loss: 2.3999\n",
            "Epoch 853/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1000 - val_loss: 2.4425\n",
            "Epoch 854/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0744 - val_loss: 2.4044\n",
            "Epoch 855/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0654 - val_loss: 2.2956\n",
            "Epoch 856/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0727 - val_loss: 2.2945\n",
            "Epoch 857/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1188 - val_loss: 2.3384\n",
            "Epoch 858/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0672 - val_loss: 2.2821\n",
            "Epoch 859/3000\n",
            "98/98 [==============================] - 1s 9ms/step - loss: 2.0568 - val_loss: 2.3444\n",
            "Epoch 860/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0935 - val_loss: 2.3876\n",
            "Epoch 861/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0637 - val_loss: 2.4232\n",
            "Epoch 862/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1453 - val_loss: 2.3744\n",
            "Epoch 863/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1213 - val_loss: 2.3907\n",
            "Epoch 864/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0336 - val_loss: 2.3863\n",
            "Epoch 865/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0594 - val_loss: 2.2896\n",
            "Epoch 866/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1707 - val_loss: 2.3481\n",
            "Epoch 867/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0705 - val_loss: 2.3471\n",
            "Epoch 868/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0757 - val_loss: 2.4001\n",
            "Epoch 869/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.1151 - val_loss: 2.4105\n",
            "Epoch 870/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1061 - val_loss: 2.2774\n",
            "Epoch 871/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0812 - val_loss: 2.2801\n",
            "Epoch 872/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1018 - val_loss: 2.2954\n",
            "Epoch 873/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0745 - val_loss: 2.3341\n",
            "Epoch 874/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0543 - val_loss: 2.4762\n",
            "Epoch 875/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0708 - val_loss: 2.3808\n",
            "Epoch 876/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.1152 - val_loss: 2.5337\n",
            "Epoch 877/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0736 - val_loss: 2.3738\n",
            "Epoch 878/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1236 - val_loss: 2.3119\n",
            "Epoch 879/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1224 - val_loss: 2.4075\n",
            "Epoch 880/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0906 - val_loss: 2.4064\n",
            "Epoch 881/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0901 - val_loss: 2.5113\n",
            "Epoch 882/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0483 - val_loss: 2.5609\n",
            "Epoch 883/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0863 - val_loss: 2.2810\n",
            "Epoch 884/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0341 - val_loss: 2.3395\n",
            "Epoch 885/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0773 - val_loss: 2.3196\n",
            "Epoch 886/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1234 - val_loss: 2.3086\n",
            "Epoch 887/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0748 - val_loss: 2.3252\n",
            "Epoch 888/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0447 - val_loss: 2.3476\n",
            "Epoch 889/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1087 - val_loss: 2.4766\n",
            "Epoch 890/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0717 - val_loss: 2.2706\n",
            "Epoch 891/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 2.0878 - val_loss: 2.2983\n",
            "Epoch 892/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0554 - val_loss: 2.3636\n",
            "Epoch 893/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0840 - val_loss: 2.4323\n",
            "Epoch 894/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1073 - val_loss: 2.3224\n",
            "Epoch 895/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1195 - val_loss: 2.4337\n",
            "Epoch 896/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1742 - val_loss: 2.3264\n",
            "Epoch 897/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0112 - val_loss: 2.5172\n",
            "Epoch 898/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0861 - val_loss: 2.3539\n",
            "Epoch 899/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1035 - val_loss: 2.3208\n",
            "Epoch 900/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1508 - val_loss: 2.3640\n",
            "Epoch 901/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1618 - val_loss: 2.2981\n",
            "Epoch 902/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1258 - val_loss: 2.2649\n",
            "Epoch 903/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0555 - val_loss: 2.2562\n",
            "Epoch 904/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0323 - val_loss: 2.3302\n",
            "Epoch 905/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0757 - val_loss: 2.3385\n",
            "Epoch 906/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0492 - val_loss: 2.2884\n",
            "Epoch 907/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0982 - val_loss: 2.3569\n",
            "Epoch 908/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0723 - val_loss: 2.4369\n",
            "Epoch 909/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0377 - val_loss: 2.3105\n",
            "Epoch 910/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0521 - val_loss: 2.2643\n",
            "Epoch 911/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0527 - val_loss: 2.4223\n",
            "Epoch 912/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0558 - val_loss: 2.2844\n",
            "Epoch 913/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1033 - val_loss: 2.3620\n",
            "Epoch 914/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0642 - val_loss: 2.5172\n",
            "Epoch 915/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0328 - val_loss: 2.4512\n",
            "Epoch 916/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0342 - val_loss: 2.3113\n",
            "Epoch 917/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0607 - val_loss: 2.5050\n",
            "Epoch 918/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0742 - val_loss: 2.4039\n",
            "Epoch 919/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0763 - val_loss: 2.4025\n",
            "Epoch 920/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1473 - val_loss: 2.5623\n",
            "Epoch 921/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0656 - val_loss: 2.2872\n",
            "Epoch 922/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0573 - val_loss: 2.4421\n",
            "Epoch 923/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0621 - val_loss: 2.2901\n",
            "Epoch 924/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0547 - val_loss: 2.2956\n",
            "Epoch 925/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0355 - val_loss: 2.3901\n",
            "Epoch 926/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.1585 - val_loss: 2.5185\n",
            "Epoch 927/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0749 - val_loss: 2.3201\n",
            "Epoch 928/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0733 - val_loss: 2.2604\n",
            "Epoch 929/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0514 - val_loss: 2.3502\n",
            "Epoch 930/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0647 - val_loss: 2.8453\n",
            "Epoch 931/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1203 - val_loss: 2.2914\n",
            "Epoch 932/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1349 - val_loss: 2.3361\n",
            "Epoch 933/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0753 - val_loss: 2.2766\n",
            "Epoch 934/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0207 - val_loss: 2.3255\n",
            "Epoch 935/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0534 - val_loss: 2.3317\n",
            "Epoch 936/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0429 - val_loss: 2.3625\n",
            "Epoch 937/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0749 - val_loss: 2.4758\n",
            "Epoch 938/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.1121 - val_loss: 2.4145\n",
            "Epoch 939/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0389 - val_loss: 2.3776\n",
            "Epoch 940/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0613 - val_loss: 2.3111\n",
            "Epoch 941/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.1007 - val_loss: 2.3713\n",
            "Epoch 942/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0803 - val_loss: 2.2943\n",
            "Epoch 943/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0640 - val_loss: 2.4214\n",
            "Epoch 944/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0830 - val_loss: 2.3972\n",
            "Epoch 945/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0530 - val_loss: 2.3085\n",
            "Epoch 946/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0324 - val_loss: 2.3373\n",
            "Epoch 947/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0557 - val_loss: 2.3202\n",
            "Epoch 948/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0672 - val_loss: 2.3675\n",
            "Epoch 949/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0213 - val_loss: 2.4975\n",
            "Epoch 950/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1022 - val_loss: 2.2776\n",
            "Epoch 951/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0102 - val_loss: 2.5082\n",
            "Epoch 952/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0312 - val_loss: 2.3204\n",
            "Epoch 953/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0528 - val_loss: 2.4178\n",
            "Epoch 954/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0725 - val_loss: 2.4963\n",
            "Epoch 955/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0730 - val_loss: 2.4357\n",
            "Epoch 956/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0367 - val_loss: 2.4434\n",
            "Epoch 957/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0767 - val_loss: 2.3536\n",
            "Epoch 958/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0839 - val_loss: 2.2663\n",
            "Epoch 959/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0318 - val_loss: 2.2810\n",
            "Epoch 960/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0576 - val_loss: 2.6255\n",
            "Epoch 961/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0756 - val_loss: 2.3177\n",
            "Epoch 962/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9984 - val_loss: 2.3924\n",
            "Epoch 963/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0186 - val_loss: 2.3428\n",
            "Epoch 964/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0762 - val_loss: 2.2881\n",
            "Epoch 965/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0531 - val_loss: 2.2902\n",
            "Epoch 966/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0495 - val_loss: 2.3721\n",
            "Epoch 967/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0953 - val_loss: 2.3807\n",
            "Epoch 968/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0256 - val_loss: 2.3015\n",
            "Epoch 969/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0633 - val_loss: 2.3367\n",
            "Epoch 970/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0464 - val_loss: 2.3223\n",
            "Epoch 971/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0543 - val_loss: 2.2873\n",
            "Epoch 972/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0004 - val_loss: 2.3540\n",
            "Epoch 973/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0349 - val_loss: 2.4139\n",
            "Epoch 974/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0524 - val_loss: 2.2859\n",
            "Epoch 975/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0368 - val_loss: 2.2354\n",
            "Epoch 976/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0429 - val_loss: 2.2893\n",
            "Epoch 977/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1107 - val_loss: 2.2585\n",
            "Epoch 978/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0299 - val_loss: 2.2800\n",
            "Epoch 979/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0270 - val_loss: 2.3315\n",
            "Epoch 980/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0329 - val_loss: 2.3487\n",
            "Epoch 981/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0435 - val_loss: 2.4821\n",
            "Epoch 982/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0461 - val_loss: 2.2636\n",
            "Epoch 983/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0446 - val_loss: 2.3321\n",
            "Epoch 984/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0158 - val_loss: 2.2776\n",
            "Epoch 985/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0525 - val_loss: 2.2845\n",
            "Epoch 986/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0133 - val_loss: 2.4885\n",
            "Epoch 987/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0334 - val_loss: 2.4071\n",
            "Epoch 988/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.1101 - val_loss: 2.4530\n",
            "Epoch 989/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0406 - val_loss: 2.2455\n",
            "Epoch 990/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0476 - val_loss: 2.3722\n",
            "Epoch 991/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0393 - val_loss: 2.2938\n",
            "Epoch 992/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0416 - val_loss: 2.3135\n",
            "Epoch 993/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.1184 - val_loss: 2.2640\n",
            "Epoch 994/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0003 - val_loss: 2.3045\n",
            "Epoch 995/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0188 - val_loss: 2.5798\n",
            "Epoch 996/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0402 - val_loss: 2.3502\n",
            "Epoch 997/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0870 - val_loss: 2.2562\n",
            "Epoch 998/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0898 - val_loss: 2.2792\n",
            "Epoch 999/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0078 - val_loss: 2.3668\n",
            "Epoch 1000/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0667 - val_loss: 2.4085\n",
            "Epoch 1001/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0651 - val_loss: 2.4345\n",
            "Epoch 1002/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0428 - val_loss: 2.2503\n",
            "Epoch 1003/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0374 - val_loss: 2.2877\n",
            "Epoch 1004/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0257 - val_loss: 2.3244\n",
            "Epoch 1005/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0460 - val_loss: 2.3119\n",
            "Epoch 1006/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0748 - val_loss: 2.3571\n",
            "Epoch 1007/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0281 - val_loss: 2.2635\n",
            "Epoch 1008/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0296 - val_loss: 2.3815\n",
            "Epoch 1009/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0561 - val_loss: 2.2820\n",
            "Epoch 1010/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0212 - val_loss: 2.5065\n",
            "Epoch 1011/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0313 - val_loss: 2.2425\n",
            "Epoch 1012/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0136 - val_loss: 2.3662\n",
            "Epoch 1013/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0167 - val_loss: 2.5303\n",
            "Epoch 1014/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0407 - val_loss: 2.3279\n",
            "Epoch 1015/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0120 - val_loss: 2.3917\n",
            "Epoch 1016/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0150 - val_loss: 2.2770\n",
            "Epoch 1017/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0401 - val_loss: 2.3165\n",
            "Epoch 1018/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0070 - val_loss: 2.3368\n",
            "Epoch 1019/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0358 - val_loss: 2.3789\n",
            "Epoch 1020/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0791 - val_loss: 2.3401\n",
            "Epoch 1021/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0542 - val_loss: 2.4733\n",
            "Epoch 1022/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0261 - val_loss: 2.2952\n",
            "Epoch 1023/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0204 - val_loss: 2.2754\n",
            "Epoch 1024/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0258 - val_loss: 2.3306\n",
            "Epoch 1025/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0395 - val_loss: 2.3706\n",
            "Epoch 1026/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0397 - val_loss: 2.3302\n",
            "Epoch 1027/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0085 - val_loss: 2.3392\n",
            "Epoch 1028/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0099 - val_loss: 2.4995\n",
            "Epoch 1029/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0753 - val_loss: 2.3064\n",
            "Epoch 1030/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0094 - val_loss: 2.3596\n",
            "Epoch 1031/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0660 - val_loss: 2.3249\n",
            "Epoch 1032/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0331 - val_loss: 2.3040\n",
            "Epoch 1033/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0449 - val_loss: 2.3287\n",
            "Epoch 1034/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0207 - val_loss: 2.3449\n",
            "Epoch 1035/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0365 - val_loss: 2.4511\n",
            "Epoch 1036/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0230 - val_loss: 2.4041\n",
            "Epoch 1037/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0633 - val_loss: 2.3646\n",
            "Epoch 1038/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0493 - val_loss: 2.4307\n",
            "Epoch 1039/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0096 - val_loss: 2.4687\n",
            "Epoch 1040/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9932 - val_loss: 2.3583\n",
            "Epoch 1041/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9948 - val_loss: 2.3710\n",
            "Epoch 1042/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0580 - val_loss: 2.2609\n",
            "Epoch 1043/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0094 - val_loss: 2.3923\n",
            "Epoch 1044/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0400 - val_loss: 2.3669\n",
            "Epoch 1045/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0008 - val_loss: 2.3027\n",
            "Epoch 1046/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0013 - val_loss: 2.2863\n",
            "Epoch 1047/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0522 - val_loss: 2.5095\n",
            "Epoch 1048/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0057 - val_loss: 2.3676\n",
            "Epoch 1049/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0782 - val_loss: 2.2774\n",
            "Epoch 1050/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0342 - val_loss: 2.3495\n",
            "Epoch 1051/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0390 - val_loss: 2.2443\n",
            "Epoch 1052/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0219 - val_loss: 2.3828\n",
            "Epoch 1053/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9925 - val_loss: 2.3281\n",
            "Epoch 1054/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9965 - val_loss: 2.2944\n",
            "Epoch 1055/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0325 - val_loss: 2.4100\n",
            "Epoch 1056/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0596 - val_loss: 2.6081\n",
            "Epoch 1057/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0212 - val_loss: 2.5231\n",
            "Epoch 1058/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0164 - val_loss: 2.3717\n",
            "Epoch 1059/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9880 - val_loss: 2.2531\n",
            "Epoch 1060/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9962 - val_loss: 2.4250\n",
            "Epoch 1061/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0395 - val_loss: 2.2757\n",
            "Epoch 1062/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9984 - val_loss: 2.2930\n",
            "Epoch 1063/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0429 - val_loss: 2.2987\n",
            "Epoch 1064/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9864 - val_loss: 2.2921\n",
            "Epoch 1065/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0199 - val_loss: 2.3337\n",
            "Epoch 1066/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0003 - val_loss: 2.4774\n",
            "Epoch 1067/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0082 - val_loss: 2.3322\n",
            "Epoch 1068/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0648 - val_loss: 2.3100\n",
            "Epoch 1069/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0549 - val_loss: 2.3826\n",
            "Epoch 1070/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9804 - val_loss: 2.3013\n",
            "Epoch 1071/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9731 - val_loss: 2.2765\n",
            "Epoch 1072/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0314 - val_loss: 2.5862\n",
            "Epoch 1073/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0624 - val_loss: 2.3600\n",
            "Epoch 1074/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9556 - val_loss: 2.3719\n",
            "Epoch 1075/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9987 - val_loss: 2.2426\n",
            "Epoch 1076/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9897 - val_loss: 2.3144\n",
            "Epoch 1077/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0409 - val_loss: 2.3188\n",
            "Epoch 1078/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9889 - val_loss: 2.3689\n",
            "Epoch 1079/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0639 - val_loss: 2.2989\n",
            "Epoch 1080/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9967 - val_loss: 2.2971\n",
            "Epoch 1081/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0009 - val_loss: 2.3816\n",
            "Epoch 1082/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 2.0404 - val_loss: 2.5224\n",
            "Epoch 1083/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0311 - val_loss: 2.2813\n",
            "Epoch 1084/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0493 - val_loss: 2.4738\n",
            "Epoch 1085/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9956 - val_loss: 2.2697\n",
            "Epoch 1086/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0131 - val_loss: 2.3084\n",
            "Epoch 1087/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9854 - val_loss: 2.4542\n",
            "Epoch 1088/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9799 - val_loss: 2.2738\n",
            "Epoch 1089/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9756 - val_loss: 2.3514\n",
            "Epoch 1090/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0070 - val_loss: 2.5342\n",
            "Epoch 1091/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0066 - val_loss: 2.3436\n",
            "Epoch 1092/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9925 - val_loss: 2.4070\n",
            "Epoch 1093/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0081 - val_loss: 2.3746\n",
            "Epoch 1094/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0061 - val_loss: 2.3891\n",
            "Epoch 1095/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9798 - val_loss: 2.3231\n",
            "Epoch 1096/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0259 - val_loss: 2.2518\n",
            "Epoch 1097/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9973 - val_loss: 2.2740\n",
            "Epoch 1098/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0242 - val_loss: 2.2742\n",
            "Epoch 1099/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9774 - val_loss: 2.3210\n",
            "Epoch 1100/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0152 - val_loss: 2.2856\n",
            "Epoch 1101/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9874 - val_loss: 2.3265\n",
            "Epoch 1102/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9896 - val_loss: 2.3349\n",
            "Epoch 1103/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9928 - val_loss: 2.3307\n",
            "Epoch 1104/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9818 - val_loss: 2.2795\n",
            "Epoch 1105/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9818 - val_loss: 2.2842\n",
            "Epoch 1106/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9842 - val_loss: 2.2387\n",
            "Epoch 1107/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0402 - val_loss: 2.2686\n",
            "Epoch 1108/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9693 - val_loss: 2.2503\n",
            "Epoch 1109/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0028 - val_loss: 2.3672\n",
            "Epoch 1110/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9775 - val_loss: 2.3215\n",
            "Epoch 1111/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9490 - val_loss: 2.2841\n",
            "Epoch 1112/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0462 - val_loss: 2.2509\n",
            "Epoch 1113/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9832 - val_loss: 2.2632\n",
            "Epoch 1114/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0377 - val_loss: 2.3684\n",
            "Epoch 1115/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9812 - val_loss: 2.3686\n",
            "Epoch 1116/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9821 - val_loss: 2.3559\n",
            "Epoch 1117/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0242 - val_loss: 2.3706\n",
            "Epoch 1118/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9943 - val_loss: 2.3788\n",
            "Epoch 1119/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9654 - val_loss: 2.2748\n",
            "Epoch 1120/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0046 - val_loss: 2.3070\n",
            "Epoch 1121/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9544 - val_loss: 2.3376\n",
            "Epoch 1122/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0433 - val_loss: 2.3725\n",
            "Epoch 1123/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 2.0205 - val_loss: 2.6282\n",
            "Epoch 1124/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9779 - val_loss: 2.4536\n",
            "Epoch 1125/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9706 - val_loss: 2.3228\n",
            "Epoch 1126/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9527 - val_loss: 2.3051\n",
            "Epoch 1127/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9869 - val_loss: 2.3426\n",
            "Epoch 1128/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0187 - val_loss: 2.4101\n",
            "Epoch 1129/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9932 - val_loss: 2.3478\n",
            "Epoch 1130/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9923 - val_loss: 2.2853\n",
            "Epoch 1131/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9646 - val_loss: 2.3052\n",
            "Epoch 1132/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9580 - val_loss: 2.5363\n",
            "Epoch 1133/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0336 - val_loss: 2.3609\n",
            "Epoch 1134/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9893 - val_loss: 2.2621\n",
            "Epoch 1135/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9684 - val_loss: 2.2711\n",
            "Epoch 1136/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9851 - val_loss: 2.3333\n",
            "Epoch 1137/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9956 - val_loss: 2.5100\n",
            "Epoch 1138/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0304 - val_loss: 2.3769\n",
            "Epoch 1139/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9413 - val_loss: 2.2841\n",
            "Epoch 1140/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9705 - val_loss: 2.3426\n",
            "Epoch 1141/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0108 - val_loss: 2.3772\n",
            "Epoch 1142/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9589 - val_loss: 2.2704\n",
            "Epoch 1143/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9446 - val_loss: 2.6650\n",
            "Epoch 1144/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9906 - val_loss: 2.2890\n",
            "Epoch 1145/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 2.0553 - val_loss: 2.2800\n",
            "Epoch 1146/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9998 - val_loss: 2.3663\n",
            "Epoch 1147/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0240 - val_loss: 2.4131\n",
            "Epoch 1148/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0557 - val_loss: 2.3243\n",
            "Epoch 1149/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9632 - val_loss: 2.2734\n",
            "Epoch 1150/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9739 - val_loss: 2.3626\n",
            "Epoch 1151/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9767 - val_loss: 2.2978\n",
            "Epoch 1152/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9660 - val_loss: 2.3399\n",
            "Epoch 1153/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9469 - val_loss: 2.2764\n",
            "Epoch 1154/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9928 - val_loss: 2.3464\n",
            "Epoch 1155/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9992 - val_loss: 2.2986\n",
            "Epoch 1156/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9726 - val_loss: 2.3885\n",
            "Epoch 1157/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9776 - val_loss: 2.2826\n",
            "Epoch 1158/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 2.0426 - val_loss: 2.4363\n",
            "Epoch 1159/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0203 - val_loss: 2.2890\n",
            "Epoch 1160/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9696 - val_loss: 2.3441\n",
            "Epoch 1161/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9768 - val_loss: 2.3877\n",
            "Epoch 1162/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9557 - val_loss: 2.3042\n",
            "Epoch 1163/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0049 - val_loss: 2.3506\n",
            "Epoch 1164/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9404 - val_loss: 2.2879\n",
            "Epoch 1165/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9912 - val_loss: 2.3088\n",
            "Epoch 1166/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9728 - val_loss: 2.3999\n",
            "Epoch 1167/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0526 - val_loss: 2.3652\n",
            "Epoch 1168/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9594 - val_loss: 2.3001\n",
            "Epoch 1169/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9734 - val_loss: 2.2959\n",
            "Epoch 1170/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9768 - val_loss: 2.2805\n",
            "Epoch 1171/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9667 - val_loss: 2.3092\n",
            "Epoch 1172/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9807 - val_loss: 2.3378\n",
            "Epoch 1173/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 2.0080 - val_loss: 2.2544\n",
            "Epoch 1174/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9443 - val_loss: 2.2722\n",
            "Epoch 1175/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9611 - val_loss: 2.2686\n",
            "Epoch 1176/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9780 - val_loss: 2.4105\n",
            "Epoch 1177/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0514 - val_loss: 2.4325\n",
            "Epoch 1178/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0606 - val_loss: 2.2979\n",
            "Epoch 1179/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9337 - val_loss: 2.3127\n",
            "Epoch 1180/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9535 - val_loss: 2.2739\n",
            "Epoch 1181/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9960 - val_loss: 2.2895\n",
            "Epoch 1182/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9576 - val_loss: 2.3417\n",
            "Epoch 1183/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 2.0315 - val_loss: 2.5262\n",
            "Epoch 1184/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9892 - val_loss: 2.2833\n",
            "Epoch 1185/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0163 - val_loss: 2.3320\n",
            "Epoch 1186/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9374 - val_loss: 2.2761\n",
            "Epoch 1187/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9925 - val_loss: 2.3708\n",
            "Epoch 1188/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9565 - val_loss: 2.2667\n",
            "Epoch 1189/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9871 - val_loss: 2.3962\n",
            "Epoch 1190/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 2.0362 - val_loss: 2.3446\n",
            "Epoch 1191/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9649 - val_loss: 2.2698\n",
            "Epoch 1192/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9750 - val_loss: 2.2825\n",
            "Epoch 1193/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0250 - val_loss: 2.3725\n",
            "Epoch 1194/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9997 - val_loss: 2.2695\n",
            "Epoch 1195/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9727 - val_loss: 2.4227\n",
            "Epoch 1196/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9503 - val_loss: 2.2443\n",
            "Epoch 1197/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9776 - val_loss: 2.2878\n",
            "Epoch 1198/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9308 - val_loss: 2.3077\n",
            "Epoch 1199/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9416 - val_loss: 2.2968\n",
            "Epoch 1200/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0463 - val_loss: 2.3131\n",
            "Epoch 1201/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9605 - val_loss: 2.3615\n",
            "Epoch 1202/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9387 - val_loss: 2.3357\n",
            "Epoch 1203/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9545 - val_loss: 2.2749\n",
            "Epoch 1204/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9833 - val_loss: 2.2940\n",
            "Epoch 1205/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9378 - val_loss: 2.2721\n",
            "Epoch 1206/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9366 - val_loss: 2.3308\n",
            "Epoch 1207/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9916 - val_loss: 2.3275\n",
            "Epoch 1208/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9804 - val_loss: 2.5179\n",
            "Epoch 1209/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9506 - val_loss: 2.3798\n",
            "Epoch 1210/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9852 - val_loss: 2.3411\n",
            "Epoch 1211/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9893 - val_loss: 2.5030\n",
            "Epoch 1212/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9823 - val_loss: 2.3613\n",
            "Epoch 1213/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9590 - val_loss: 2.3072\n",
            "Epoch 1214/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9839 - val_loss: 2.3972\n",
            "Epoch 1215/3000\n",
            "98/98 [==============================] - 1s 10ms/step - loss: 1.9645 - val_loss: 2.3613\n",
            "Epoch 1216/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9304 - val_loss: 2.4071\n",
            "Epoch 1217/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9811 - val_loss: 2.2760\n",
            "Epoch 1218/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9637 - val_loss: 2.3615\n",
            "Epoch 1219/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9480 - val_loss: 2.3084\n",
            "Epoch 1220/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9440 - val_loss: 2.3158\n",
            "Epoch 1221/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9268 - val_loss: 2.3820\n",
            "Epoch 1222/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0086 - val_loss: 2.4454\n",
            "Epoch 1223/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9754 - val_loss: 2.3678\n",
            "Epoch 1224/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9507 - val_loss: 2.3441\n",
            "Epoch 1225/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9429 - val_loss: 2.4771\n",
            "Epoch 1226/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9280 - val_loss: 2.2849\n",
            "Epoch 1227/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9839 - val_loss: 2.2970\n",
            "Epoch 1228/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9522 - val_loss: 2.3014\n",
            "Epoch 1229/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9543 - val_loss: 2.4457\n",
            "Epoch 1230/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0031 - val_loss: 2.3464\n",
            "Epoch 1231/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9420 - val_loss: 2.2885\n",
            "Epoch 1232/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9541 - val_loss: 2.2746\n",
            "Epoch 1233/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9814 - val_loss: 2.3189\n",
            "Epoch 1234/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9377 - val_loss: 2.2661\n",
            "Epoch 1235/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9453 - val_loss: 2.3689\n",
            "Epoch 1236/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9614 - val_loss: 2.2832\n",
            "Epoch 1237/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9963 - val_loss: 2.2812\n",
            "Epoch 1238/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0153 - val_loss: 2.4848\n",
            "Epoch 1239/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9681 - val_loss: 2.2980\n",
            "Epoch 1240/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9619 - val_loss: 2.2595\n",
            "Epoch 1241/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9553 - val_loss: 2.4648\n",
            "Epoch 1242/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9487 - val_loss: 2.2756\n",
            "Epoch 1243/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9649 - val_loss: 2.3999\n",
            "Epoch 1244/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0057 - val_loss: 2.2683\n",
            "Epoch 1245/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9362 - val_loss: 2.4225\n",
            "Epoch 1246/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9422 - val_loss: 2.4224\n",
            "Epoch 1247/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9775 - val_loss: 2.4778\n",
            "Epoch 1248/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9664 - val_loss: 2.3346\n",
            "Epoch 1249/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9447 - val_loss: 2.3994\n",
            "Epoch 1250/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9642 - val_loss: 2.3513\n",
            "Epoch 1251/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9490 - val_loss: 2.2726\n",
            "Epoch 1252/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9475 - val_loss: 2.5111\n",
            "Epoch 1253/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9313 - val_loss: 2.2898\n",
            "Epoch 1254/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9483 - val_loss: 2.3707\n",
            "Epoch 1255/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9617 - val_loss: 2.3071\n",
            "Epoch 1256/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9356 - val_loss: 2.3912\n",
            "Epoch 1257/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9220 - val_loss: 2.3140\n",
            "Epoch 1258/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9515 - val_loss: 2.4522\n",
            "Epoch 1259/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9507 - val_loss: 2.2934\n",
            "Epoch 1260/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9502 - val_loss: 2.3336\n",
            "Epoch 1261/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9438 - val_loss: 2.2735\n",
            "Epoch 1262/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9627 - val_loss: 2.4192\n",
            "Epoch 1263/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9632 - val_loss: 2.2661\n",
            "Epoch 1264/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9284 - val_loss: 2.4670\n",
            "Epoch 1265/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9505 - val_loss: 2.2959\n",
            "Epoch 1266/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9900 - val_loss: 2.3233\n",
            "Epoch 1267/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9423 - val_loss: 2.3041\n",
            "Epoch 1268/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9607 - val_loss: 2.4298\n",
            "Epoch 1269/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9102 - val_loss: 2.3373\n",
            "Epoch 1270/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9324 - val_loss: 2.3046\n",
            "Epoch 1271/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9315 - val_loss: 2.3778\n",
            "Epoch 1272/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9100 - val_loss: 2.3857\n",
            "Epoch 1273/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9914 - val_loss: 2.3100\n",
            "Epoch 1274/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9274 - val_loss: 2.3429\n",
            "Epoch 1275/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9889 - val_loss: 2.2923\n",
            "Epoch 1276/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9074 - val_loss: 2.3154\n",
            "Epoch 1277/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 2.0399 - val_loss: 2.3351\n",
            "Epoch 1278/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9867 - val_loss: 2.5591\n",
            "Epoch 1279/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9531 - val_loss: 2.3223\n",
            "Epoch 1280/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9129 - val_loss: 2.4747\n",
            "Epoch 1281/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0601 - val_loss: 2.3450\n",
            "Epoch 1282/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9083 - val_loss: 2.3450\n",
            "Epoch 1283/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9501 - val_loss: 2.6739\n",
            "Epoch 1284/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9565 - val_loss: 2.3129\n",
            "Epoch 1285/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9741 - val_loss: 2.4096\n",
            "Epoch 1286/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0161 - val_loss: 2.2592\n",
            "Epoch 1287/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9054 - val_loss: 2.3897\n",
            "Epoch 1288/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9741 - val_loss: 2.4092\n",
            "Epoch 1289/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9374 - val_loss: 2.4598\n",
            "Epoch 1290/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 2.0116 - val_loss: 2.3418\n",
            "Epoch 1291/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9216 - val_loss: 2.3008\n",
            "Epoch 1292/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9164 - val_loss: 2.3119\n",
            "Epoch 1293/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9557 - val_loss: 2.3006\n",
            "Epoch 1294/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9428 - val_loss: 2.2833\n",
            "Epoch 1295/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9635 - val_loss: 2.2628\n",
            "Epoch 1296/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9429 - val_loss: 2.3014\n",
            "Epoch 1297/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9125 - val_loss: 2.3267\n",
            "Epoch 1298/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8990 - val_loss: 2.2971\n",
            "Epoch 1299/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9111 - val_loss: 2.3395\n",
            "Epoch 1300/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9071 - val_loss: 2.3236\n",
            "Epoch 1301/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9481 - val_loss: 2.3736\n",
            "Epoch 1302/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9313 - val_loss: 2.3181\n",
            "Epoch 1303/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.9142 - val_loss: 2.3177\n",
            "Epoch 1304/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9276 - val_loss: 2.4109\n",
            "Epoch 1305/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0063 - val_loss: 2.3187\n",
            "Epoch 1306/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9276 - val_loss: 2.2941\n",
            "Epoch 1307/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9343 - val_loss: 2.4390\n",
            "Epoch 1308/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9676 - val_loss: 2.3681\n",
            "Epoch 1309/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9211 - val_loss: 2.3195\n",
            "Epoch 1310/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9160 - val_loss: 2.2734\n",
            "Epoch 1311/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9006 - val_loss: 2.5205\n",
            "Epoch 1312/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9810 - val_loss: 2.2978\n",
            "Epoch 1313/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9078 - val_loss: 2.3104\n",
            "Epoch 1314/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9195 - val_loss: 2.3443\n",
            "Epoch 1315/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9063 - val_loss: 2.2969\n",
            "Epoch 1316/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9520 - val_loss: 2.3558\n",
            "Epoch 1317/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9575 - val_loss: 2.3196\n",
            "Epoch 1318/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9029 - val_loss: 2.2780\n",
            "Epoch 1319/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9356 - val_loss: 2.3074\n",
            "Epoch 1320/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9342 - val_loss: 2.4490\n",
            "Epoch 1321/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9047 - val_loss: 2.3580\n",
            "Epoch 1322/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9365 - val_loss: 2.2986\n",
            "Epoch 1323/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9474 - val_loss: 2.4480\n",
            "Epoch 1324/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9936 - val_loss: 2.3564\n",
            "Epoch 1325/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9400 - val_loss: 2.2775\n",
            "Epoch 1326/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9703 - val_loss: 2.4001\n",
            "Epoch 1327/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9446 - val_loss: 2.3000\n",
            "Epoch 1328/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8862 - val_loss: 2.2964\n",
            "Epoch 1329/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9149 - val_loss: 2.2785\n",
            "Epoch 1330/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9200 - val_loss: 2.3538\n",
            "Epoch 1331/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9070 - val_loss: 2.3812\n",
            "Epoch 1332/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9064 - val_loss: 2.3272\n",
            "Epoch 1333/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8999 - val_loss: 2.2845\n",
            "Epoch 1334/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 2.0105 - val_loss: 2.5661\n",
            "Epoch 1335/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9389 - val_loss: 2.3380\n",
            "Epoch 1336/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9339 - val_loss: 2.3821\n",
            "Epoch 1337/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9238 - val_loss: 2.3073\n",
            "Epoch 1338/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9385 - val_loss: 2.4060\n",
            "Epoch 1339/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9495 - val_loss: 2.3580\n",
            "Epoch 1340/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9088 - val_loss: 2.3103\n",
            "Epoch 1341/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9174 - val_loss: 2.4205\n",
            "Epoch 1342/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9607 - val_loss: 2.2884\n",
            "Epoch 1343/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9236 - val_loss: 2.3601\n",
            "Epoch 1344/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9372 - val_loss: 2.4882\n",
            "Epoch 1345/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9233 - val_loss: 2.3522\n",
            "Epoch 1346/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9333 - val_loss: 2.3350\n",
            "Epoch 1347/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9099 - val_loss: 2.2874\n",
            "Epoch 1348/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9194 - val_loss: 2.5302\n",
            "Epoch 1349/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9249 - val_loss: 2.3869\n",
            "Epoch 1350/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8852 - val_loss: 2.3819\n",
            "Epoch 1351/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9401 - val_loss: 2.3164\n",
            "Epoch 1352/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9867 - val_loss: 2.3103\n",
            "Epoch 1353/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9911 - val_loss: 2.3497\n",
            "Epoch 1354/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8966 - val_loss: 2.3185\n",
            "Epoch 1355/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9281 - val_loss: 2.2914\n",
            "Epoch 1356/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9061 - val_loss: 2.4011\n",
            "Epoch 1357/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9072 - val_loss: 2.3957\n",
            "Epoch 1358/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9324 - val_loss: 2.2565\n",
            "Epoch 1359/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8850 - val_loss: 2.4337\n",
            "Epoch 1360/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9379 - val_loss: 2.3461\n",
            "Epoch 1361/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9817 - val_loss: 2.3231\n",
            "Epoch 1362/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9541 - val_loss: 2.2826\n",
            "Epoch 1363/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9522 - val_loss: 2.3154\n",
            "Epoch 1364/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9023 - val_loss: 2.2870\n",
            "Epoch 1365/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8921 - val_loss: 2.3156\n",
            "Epoch 1366/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8990 - val_loss: 2.2624\n",
            "Epoch 1367/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8949 - val_loss: 2.3579\n",
            "Epoch 1368/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9082 - val_loss: 2.2423\n",
            "Epoch 1369/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9142 - val_loss: 2.2909\n",
            "Epoch 1370/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9543 - val_loss: 2.2890\n",
            "Epoch 1371/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9098 - val_loss: 2.3370\n",
            "Epoch 1372/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9709 - val_loss: 2.2917\n",
            "Epoch 1373/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9015 - val_loss: 2.3799\n",
            "Epoch 1374/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9289 - val_loss: 2.3460\n",
            "Epoch 1375/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8958 - val_loss: 2.3828\n",
            "Epoch 1376/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9983 - val_loss: 2.3710\n",
            "Epoch 1377/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8949 - val_loss: 2.3298\n",
            "Epoch 1378/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9044 - val_loss: 2.3129\n",
            "Epoch 1379/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8937 - val_loss: 2.3469\n",
            "Epoch 1380/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8910 - val_loss: 2.4842\n",
            "Epoch 1381/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9245 - val_loss: 2.3015\n",
            "Epoch 1382/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8885 - val_loss: 2.2674\n",
            "Epoch 1383/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9063 - val_loss: 2.2990\n",
            "Epoch 1384/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8806 - val_loss: 2.2959\n",
            "Epoch 1385/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9502 - val_loss: 2.2983\n",
            "Epoch 1386/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9076 - val_loss: 2.3316\n",
            "Epoch 1387/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8769 - val_loss: 2.2740\n",
            "Epoch 1388/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9008 - val_loss: 2.3465\n",
            "Epoch 1389/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9115 - val_loss: 2.3226\n",
            "Epoch 1390/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9195 - val_loss: 2.3325\n",
            "Epoch 1391/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9662 - val_loss: 2.3140\n",
            "Epoch 1392/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9132 - val_loss: 2.3068\n",
            "Epoch 1393/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9228 - val_loss: 2.2688\n",
            "Epoch 1394/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9132 - val_loss: 2.2636\n",
            "Epoch 1395/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9284 - val_loss: 2.5573\n",
            "Epoch 1396/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.9133 - val_loss: 2.3837\n",
            "Epoch 1397/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9032 - val_loss: 2.2952\n",
            "Epoch 1398/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9622 - val_loss: 2.3055\n",
            "Epoch 1399/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8907 - val_loss: 2.3046\n",
            "Epoch 1400/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8752 - val_loss: 2.3926\n",
            "Epoch 1401/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9358 - val_loss: 2.2658\n",
            "Epoch 1402/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9107 - val_loss: 2.2996\n",
            "Epoch 1403/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9237 - val_loss: 2.3251\n",
            "Epoch 1404/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9880 - val_loss: 2.3586\n",
            "Epoch 1405/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8950 - val_loss: 2.3657\n",
            "Epoch 1406/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9085 - val_loss: 2.4861\n",
            "Epoch 1407/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9104 - val_loss: 2.3161\n",
            "Epoch 1408/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.9273 - val_loss: 2.2648\n",
            "Epoch 1409/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8952 - val_loss: 2.2456\n",
            "Epoch 1410/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8682 - val_loss: 2.2781\n",
            "Epoch 1411/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9169 - val_loss: 2.3367\n",
            "Epoch 1412/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9014 - val_loss: 2.3212\n",
            "Epoch 1413/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9141 - val_loss: 2.3271\n",
            "Epoch 1414/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8912 - val_loss: 2.3048\n",
            "Epoch 1415/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8903 - val_loss: 2.3242\n",
            "Epoch 1416/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8891 - val_loss: 2.5567\n",
            "Epoch 1417/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9638 - val_loss: 2.4064\n",
            "Epoch 1418/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9015 - val_loss: 2.3588\n",
            "Epoch 1419/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9166 - val_loss: 2.3553\n",
            "Epoch 1420/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8865 - val_loss: 2.2975\n",
            "Epoch 1421/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9139 - val_loss: 2.4345\n",
            "Epoch 1422/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.9082 - val_loss: 2.3742\n",
            "Epoch 1423/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8958 - val_loss: 2.2722\n",
            "Epoch 1424/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9334 - val_loss: 2.3143\n",
            "Epoch 1425/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9142 - val_loss: 2.3138\n",
            "Epoch 1426/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8668 - val_loss: 2.3190\n",
            "Epoch 1427/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9565 - val_loss: 2.3276\n",
            "Epoch 1428/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8930 - val_loss: 2.2805\n",
            "Epoch 1429/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9634 - val_loss: 2.2850\n",
            "Epoch 1430/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8779 - val_loss: 2.2919\n",
            "Epoch 1431/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8802 - val_loss: 2.2693\n",
            "Epoch 1432/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8961 - val_loss: 2.3479\n",
            "Epoch 1433/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9015 - val_loss: 2.5306\n",
            "Epoch 1434/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8849 - val_loss: 2.3703\n",
            "Epoch 1435/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9079 - val_loss: 2.3770\n",
            "Epoch 1436/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8853 - val_loss: 2.2970\n",
            "Epoch 1437/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9034 - val_loss: 2.5691\n",
            "Epoch 1438/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9232 - val_loss: 2.2826\n",
            "Epoch 1439/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8980 - val_loss: 2.3067\n",
            "Epoch 1440/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9044 - val_loss: 2.3706\n",
            "Epoch 1441/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9439 - val_loss: 2.5094\n",
            "Epoch 1442/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9086 - val_loss: 2.3634\n",
            "Epoch 1443/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8962 - val_loss: 2.4069\n",
            "Epoch 1444/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.9604 - val_loss: 2.2927\n",
            "Epoch 1445/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8833 - val_loss: 2.3337\n",
            "Epoch 1446/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8783 - val_loss: 2.3078\n",
            "Epoch 1447/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8846 - val_loss: 2.4174\n",
            "Epoch 1448/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8626 - val_loss: 2.3157\n",
            "Epoch 1449/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8889 - val_loss: 2.4072\n",
            "Epoch 1450/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9045 - val_loss: 2.2580\n",
            "Epoch 1451/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9640 - val_loss: 2.3058\n",
            "Epoch 1452/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8830 - val_loss: 2.2902\n",
            "Epoch 1453/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9342 - val_loss: 2.2946\n",
            "Epoch 1454/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8742 - val_loss: 2.2625\n",
            "Epoch 1455/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9239 - val_loss: 2.3059\n",
            "Epoch 1456/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8934 - val_loss: 2.2805\n",
            "Epoch 1457/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9202 - val_loss: 2.3327\n",
            "Epoch 1458/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8882 - val_loss: 2.3988\n",
            "Epoch 1459/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9338 - val_loss: 2.2802\n",
            "Epoch 1460/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8912 - val_loss: 2.2969\n",
            "Epoch 1461/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8821 - val_loss: 2.2919\n",
            "Epoch 1462/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9105 - val_loss: 2.3321\n",
            "Epoch 1463/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8795 - val_loss: 2.2849\n",
            "Epoch 1464/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8752 - val_loss: 2.3931\n",
            "Epoch 1465/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8952 - val_loss: 2.3952\n",
            "Epoch 1466/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8677 - val_loss: 2.2666\n",
            "Epoch 1467/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8533 - val_loss: 2.2724\n",
            "Epoch 1468/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8702 - val_loss: 2.2664\n",
            "Epoch 1469/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.9040 - val_loss: 2.4477\n",
            "Epoch 1470/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8746 - val_loss: 2.2692\n",
            "Epoch 1471/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9048 - val_loss: 2.2629\n",
            "Epoch 1472/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8905 - val_loss: 2.3922\n",
            "Epoch 1473/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9106 - val_loss: 2.6596\n",
            "Epoch 1474/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9959 - val_loss: 2.3122\n",
            "Epoch 1475/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9057 - val_loss: 2.2845\n",
            "Epoch 1476/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8754 - val_loss: 2.3031\n",
            "Epoch 1477/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8731 - val_loss: 2.3075\n",
            "Epoch 1478/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8992 - val_loss: 2.6106\n",
            "Epoch 1479/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9568 - val_loss: 2.9691\n",
            "Epoch 1480/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9348 - val_loss: 2.3132\n",
            "Epoch 1481/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9115 - val_loss: 2.3278\n",
            "Epoch 1482/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8698 - val_loss: 2.2901\n",
            "Epoch 1483/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8590 - val_loss: 2.3495\n",
            "Epoch 1484/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8510 - val_loss: 2.5000\n",
            "Epoch 1485/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9362 - val_loss: 2.4031\n",
            "Epoch 1486/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9319 - val_loss: 2.3200\n",
            "Epoch 1487/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9072 - val_loss: 2.3242\n",
            "Epoch 1488/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9201 - val_loss: 2.3180\n",
            "Epoch 1489/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8842 - val_loss: 2.4572\n",
            "Epoch 1490/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8939 - val_loss: 2.3113\n",
            "Epoch 1491/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8894 - val_loss: 2.3309\n",
            "Epoch 1492/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8874 - val_loss: 2.3080\n",
            "Epoch 1493/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9184 - val_loss: 2.2786\n",
            "Epoch 1494/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8622 - val_loss: 2.3173\n",
            "Epoch 1495/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9456 - val_loss: 2.3083\n",
            "Epoch 1496/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8739 - val_loss: 2.3245\n",
            "Epoch 1497/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8656 - val_loss: 2.2998\n",
            "Epoch 1498/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9635 - val_loss: 2.4420\n",
            "Epoch 1499/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9251 - val_loss: 2.3759\n",
            "Epoch 1500/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8609 - val_loss: 2.3225\n",
            "Epoch 1501/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8500 - val_loss: 2.3485\n",
            "Epoch 1502/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9233 - val_loss: 2.3477\n",
            "Epoch 1503/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8634 - val_loss: 2.3166\n",
            "Epoch 1504/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8774 - val_loss: 2.3467\n",
            "Epoch 1505/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9576 - val_loss: 2.2847\n",
            "Epoch 1506/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8851 - val_loss: 2.2688\n",
            "Epoch 1507/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8441 - val_loss: 2.4409\n",
            "Epoch 1508/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.9473 - val_loss: 2.2699\n",
            "Epoch 1509/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8735 - val_loss: 2.3223\n",
            "Epoch 1510/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8714 - val_loss: 2.2681\n",
            "Epoch 1511/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8725 - val_loss: 2.2534\n",
            "Epoch 1512/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8743 - val_loss: 2.2931\n",
            "Epoch 1513/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9295 - val_loss: 2.2781\n",
            "Epoch 1514/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8789 - val_loss: 2.2609\n",
            "Epoch 1515/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8620 - val_loss: 2.4067\n",
            "Epoch 1516/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8673 - val_loss: 2.3048\n",
            "Epoch 1517/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8516 - val_loss: 2.2862\n",
            "Epoch 1518/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8431 - val_loss: 2.2726\n",
            "Epoch 1519/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8679 - val_loss: 2.3939\n",
            "Epoch 1520/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8821 - val_loss: 2.2804\n",
            "Epoch 1521/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8507 - val_loss: 2.3893\n",
            "Epoch 1522/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8555 - val_loss: 2.3603\n",
            "Epoch 1523/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9073 - val_loss: 2.2749\n",
            "Epoch 1524/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8734 - val_loss: 2.2968\n",
            "Epoch 1525/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8817 - val_loss: 2.3325\n",
            "Epoch 1526/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8580 - val_loss: 2.3648\n",
            "Epoch 1527/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8958 - val_loss: 2.2784\n",
            "Epoch 1528/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8847 - val_loss: 2.4254\n",
            "Epoch 1529/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8612 - val_loss: 2.2920\n",
            "Epoch 1530/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8585 - val_loss: 2.3437\n",
            "Epoch 1531/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8558 - val_loss: 2.3054\n",
            "Epoch 1532/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8654 - val_loss: 2.3392\n",
            "Epoch 1533/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8899 - val_loss: 2.2910\n",
            "Epoch 1534/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8392 - val_loss: 2.3570\n",
            "Epoch 1535/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8710 - val_loss: 2.4047\n",
            "Epoch 1536/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8971 - val_loss: 2.3252\n",
            "Epoch 1537/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9102 - val_loss: 2.3649\n",
            "Epoch 1538/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9086 - val_loss: 2.4286\n",
            "Epoch 1539/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8481 - val_loss: 2.2807\n",
            "Epoch 1540/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8643 - val_loss: 2.3124\n",
            "Epoch 1541/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8938 - val_loss: 2.4063\n",
            "Epoch 1542/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8690 - val_loss: 2.3421\n",
            "Epoch 1543/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8785 - val_loss: 2.3061\n",
            "Epoch 1544/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8470 - val_loss: 2.3260\n",
            "Epoch 1545/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8756 - val_loss: 2.2903\n",
            "Epoch 1546/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.9385 - val_loss: 2.3713\n",
            "Epoch 1547/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8759 - val_loss: 2.3107\n",
            "Epoch 1548/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8806 - val_loss: 2.3728\n",
            "Epoch 1549/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8616 - val_loss: 2.2738\n",
            "Epoch 1550/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8715 - val_loss: 2.3082\n",
            "Epoch 1551/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8830 - val_loss: 2.4233\n",
            "Epoch 1552/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8931 - val_loss: 2.4712\n",
            "Epoch 1553/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8693 - val_loss: 2.3055\n",
            "Epoch 1554/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8505 - val_loss: 2.2762\n",
            "Epoch 1555/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8880 - val_loss: 2.3175\n",
            "Epoch 1556/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8493 - val_loss: 2.2912\n",
            "Epoch 1557/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8969 - val_loss: 2.3231\n",
            "Epoch 1558/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8660 - val_loss: 2.2981\n",
            "Epoch 1559/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8472 - val_loss: 2.2872\n",
            "Epoch 1560/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8722 - val_loss: 2.4736\n",
            "Epoch 1561/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.9221 - val_loss: 2.3166\n",
            "Epoch 1562/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8703 - val_loss: 2.3338\n",
            "Epoch 1563/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8743 - val_loss: 2.2713\n",
            "Epoch 1564/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8900 - val_loss: 2.3914\n",
            "Epoch 1565/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.9206 - val_loss: 2.4012\n",
            "Epoch 1566/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8781 - val_loss: 2.2718\n",
            "Epoch 1567/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8653 - val_loss: 2.3318\n",
            "Epoch 1568/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8795 - val_loss: 2.3071\n",
            "Epoch 1569/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8347 - val_loss: 2.2522\n",
            "Epoch 1570/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8526 - val_loss: 2.2612\n",
            "Epoch 1571/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8396 - val_loss: 2.2737\n",
            "Epoch 1572/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8888 - val_loss: 2.3026\n",
            "Epoch 1573/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8457 - val_loss: 2.3556\n",
            "Epoch 1574/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8713 - val_loss: 2.3750\n",
            "Epoch 1575/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8814 - val_loss: 2.3082\n",
            "Epoch 1576/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8711 - val_loss: 2.4331\n",
            "Epoch 1577/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8451 - val_loss: 2.3988\n",
            "Epoch 1578/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8654 - val_loss: 2.3003\n",
            "Epoch 1579/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8325 - val_loss: 2.2741\n",
            "Epoch 1580/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8584 - val_loss: 2.3131\n",
            "Epoch 1581/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8641 - val_loss: 2.3810\n",
            "Epoch 1582/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8643 - val_loss: 2.2902\n",
            "Epoch 1583/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8365 - val_loss: 2.2947\n",
            "Epoch 1584/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8972 - val_loss: 2.3428\n",
            "Epoch 1585/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.9089 - val_loss: 2.4932\n",
            "Epoch 1586/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8854 - val_loss: 2.2590\n",
            "Epoch 1587/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8300 - val_loss: 2.3785\n",
            "Epoch 1588/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8724 - val_loss: 2.3684\n",
            "Epoch 1589/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8529 - val_loss: 2.2882\n",
            "Epoch 1590/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8519 - val_loss: 2.3957\n",
            "Epoch 1591/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8369 - val_loss: 2.2715\n",
            "Epoch 1592/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8681 - val_loss: 2.5004\n",
            "Epoch 1593/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8886 - val_loss: 2.2713\n",
            "Epoch 1594/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8741 - val_loss: 2.3144\n",
            "Epoch 1595/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8348 - val_loss: 2.2836\n",
            "Epoch 1596/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8361 - val_loss: 2.3942\n",
            "Epoch 1597/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8576 - val_loss: 2.2766\n",
            "Epoch 1598/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8803 - val_loss: 2.3662\n",
            "Epoch 1599/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.9161 - val_loss: 2.3066\n",
            "Epoch 1600/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8411 - val_loss: 2.2877\n",
            "Epoch 1601/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8492 - val_loss: 2.3305\n",
            "Epoch 1602/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8393 - val_loss: 2.3271\n",
            "Epoch 1603/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8391 - val_loss: 2.3557\n",
            "Epoch 1604/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8716 - val_loss: 2.3229\n",
            "Epoch 1605/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8143 - val_loss: 2.2944\n",
            "Epoch 1606/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8210 - val_loss: 2.2763\n",
            "Epoch 1607/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8985 - val_loss: 2.2708\n",
            "Epoch 1608/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8289 - val_loss: 2.2946\n",
            "Epoch 1609/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8160 - val_loss: 2.2936\n",
            "Epoch 1610/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8705 - val_loss: 2.3364\n",
            "Epoch 1611/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8337 - val_loss: 2.2943\n",
            "Epoch 1612/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8702 - val_loss: 2.3123\n",
            "Epoch 1613/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8665 - val_loss: 2.2784\n",
            "Epoch 1614/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8482 - val_loss: 2.2755\n",
            "Epoch 1615/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8768 - val_loss: 2.3725\n",
            "Epoch 1616/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8659 - val_loss: 2.2997\n",
            "Epoch 1617/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8684 - val_loss: 2.2977\n",
            "Epoch 1618/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8762 - val_loss: 2.2891\n",
            "Epoch 1619/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8703 - val_loss: 2.4340\n",
            "Epoch 1620/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8461 - val_loss: 2.2819\n",
            "Epoch 1621/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8153 - val_loss: 2.2796\n",
            "Epoch 1622/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8583 - val_loss: 2.3515\n",
            "Epoch 1623/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8456 - val_loss: 2.2791\n",
            "Epoch 1624/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8748 - val_loss: 2.2824\n",
            "Epoch 1625/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8503 - val_loss: 2.3347\n",
            "Epoch 1626/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8561 - val_loss: 2.3323\n",
            "Epoch 1627/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8018 - val_loss: 2.2548\n",
            "Epoch 1628/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8386 - val_loss: 2.2896\n",
            "Epoch 1629/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8267 - val_loss: 2.3173\n",
            "Epoch 1630/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8183 - val_loss: 2.3203\n",
            "Epoch 1631/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8889 - val_loss: 2.3198\n",
            "Epoch 1632/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8474 - val_loss: 2.3016\n",
            "Epoch 1633/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8561 - val_loss: 2.3677\n",
            "Epoch 1634/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8511 - val_loss: 2.3154\n",
            "Epoch 1635/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8247 - val_loss: 2.4225\n",
            "Epoch 1636/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8833 - val_loss: 2.2860\n",
            "Epoch 1637/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8743 - val_loss: 2.3005\n",
            "Epoch 1638/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8193 - val_loss: 2.3231\n",
            "Epoch 1639/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8386 - val_loss: 2.3791\n",
            "Epoch 1640/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8142 - val_loss: 2.4041\n",
            "Epoch 1641/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8476 - val_loss: 2.2910\n",
            "Epoch 1642/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8431 - val_loss: 2.3840\n",
            "Epoch 1643/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8242 - val_loss: 2.3478\n",
            "Epoch 1644/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8190 - val_loss: 2.3353\n",
            "Epoch 1645/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8563 - val_loss: 2.4597\n",
            "Epoch 1646/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8523 - val_loss: 2.3126\n",
            "Epoch 1647/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8262 - val_loss: 2.3028\n",
            "Epoch 1648/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8342 - val_loss: 2.3373\n",
            "Epoch 1649/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8785 - val_loss: 2.3902\n",
            "Epoch 1650/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8513 - val_loss: 2.5036\n",
            "Epoch 1651/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8909 - val_loss: 2.4276\n",
            "Epoch 1652/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8335 - val_loss: 2.2813\n",
            "Epoch 1653/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8274 - val_loss: 2.4084\n",
            "Epoch 1654/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8791 - val_loss: 2.3178\n",
            "Epoch 1655/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8464 - val_loss: 2.3171\n",
            "Epoch 1656/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8333 - val_loss: 2.3098\n",
            "Epoch 1657/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8326 - val_loss: 2.3297\n",
            "Epoch 1658/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8247 - val_loss: 2.3146\n",
            "Epoch 1659/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8882 - val_loss: 2.3284\n",
            "Epoch 1660/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8793 - val_loss: 2.3664\n",
            "Epoch 1661/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8290 - val_loss: 2.2821\n",
            "Epoch 1662/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8422 - val_loss: 2.3322\n",
            "Epoch 1663/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8629 - val_loss: 2.3577\n",
            "Epoch 1664/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8175 - val_loss: 2.3543\n",
            "Epoch 1665/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8230 - val_loss: 2.2539\n",
            "Epoch 1666/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8254 - val_loss: 2.2943\n",
            "Epoch 1667/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8347 - val_loss: 2.3175\n",
            "Epoch 1668/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8472 - val_loss: 2.2818\n",
            "Epoch 1669/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8511 - val_loss: 2.3907\n",
            "Epoch 1670/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8670 - val_loss: 2.2946\n",
            "Epoch 1671/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8183 - val_loss: 2.4088\n",
            "Epoch 1672/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8144 - val_loss: 2.2810\n",
            "Epoch 1673/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8547 - val_loss: 2.3671\n",
            "Epoch 1674/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8462 - val_loss: 2.3096\n",
            "Epoch 1675/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8473 - val_loss: 2.3809\n",
            "Epoch 1676/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8592 - val_loss: 2.2480\n",
            "Epoch 1677/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8069 - val_loss: 2.2904\n",
            "Epoch 1678/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8498 - val_loss: 2.3434\n",
            "Epoch 1679/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8368 - val_loss: 2.2902\n",
            "Epoch 1680/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.8068 - val_loss: 2.2878\n",
            "Epoch 1681/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8420 - val_loss: 2.3269\n",
            "Epoch 1682/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8816 - val_loss: 2.2629\n",
            "Epoch 1683/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8191 - val_loss: 2.3442\n",
            "Epoch 1684/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8178 - val_loss: 2.3746\n",
            "Epoch 1685/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8688 - val_loss: 2.4290\n",
            "Epoch 1686/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8339 - val_loss: 2.3074\n",
            "Epoch 1687/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8163 - val_loss: 2.3110\n",
            "Epoch 1688/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8226 - val_loss: 2.3441\n",
            "Epoch 1689/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8560 - val_loss: 2.3189\n",
            "Epoch 1690/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8015 - val_loss: 2.2991\n",
            "Epoch 1691/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8447 - val_loss: 2.2799\n",
            "Epoch 1692/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8118 - val_loss: 2.4190\n",
            "Epoch 1693/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8451 - val_loss: 2.4089\n",
            "Epoch 1694/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8535 - val_loss: 2.3759\n",
            "Epoch 1695/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8344 - val_loss: 2.3490\n",
            "Epoch 1696/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8323 - val_loss: 2.3643\n",
            "Epoch 1697/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8554 - val_loss: 2.3144\n",
            "Epoch 1698/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8152 - val_loss: 2.2866\n",
            "Epoch 1699/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8653 - val_loss: 2.3136\n",
            "Epoch 1700/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8136 - val_loss: 2.3578\n",
            "Epoch 1701/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8503 - val_loss: 2.5073\n",
            "Epoch 1702/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8709 - val_loss: 2.4753\n",
            "Epoch 1703/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8825 - val_loss: 2.4970\n",
            "Epoch 1704/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8329 - val_loss: 2.3060\n",
            "Epoch 1705/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8157 - val_loss: 2.4076\n",
            "Epoch 1706/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8352 - val_loss: 2.3145\n",
            "Epoch 1707/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8148 - val_loss: 2.3335\n",
            "Epoch 1708/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8301 - val_loss: 2.4090\n",
            "Epoch 1709/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8258 - val_loss: 2.2891\n",
            "Epoch 1710/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8206 - val_loss: 2.3814\n",
            "Epoch 1711/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8468 - val_loss: 2.3902\n",
            "Epoch 1712/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8320 - val_loss: 2.3901\n",
            "Epoch 1713/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8254 - val_loss: 2.3645\n",
            "Epoch 1714/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7977 - val_loss: 2.2915\n",
            "Epoch 1715/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8551 - val_loss: 2.5620\n",
            "Epoch 1716/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8699 - val_loss: 2.3300\n",
            "Epoch 1717/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8091 - val_loss: 2.4681\n",
            "Epoch 1718/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8207 - val_loss: 2.3018\n",
            "Epoch 1719/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8016 - val_loss: 2.3213\n",
            "Epoch 1720/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8411 - val_loss: 2.4009\n",
            "Epoch 1721/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8362 - val_loss: 2.3405\n",
            "Epoch 1722/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8545 - val_loss: 2.3261\n",
            "Epoch 1723/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8280 - val_loss: 2.3505\n",
            "Epoch 1724/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8147 - val_loss: 2.2929\n",
            "Epoch 1725/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.8739 - val_loss: 2.2841\n",
            "Epoch 1726/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8333 - val_loss: 2.3191\n",
            "Epoch 1727/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8560 - val_loss: 2.3138\n",
            "Epoch 1728/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7931 - val_loss: 2.2829\n",
            "Epoch 1729/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7971 - val_loss: 2.2678\n",
            "Epoch 1730/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8073 - val_loss: 2.2623\n",
            "Epoch 1731/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8137 - val_loss: 2.3028\n",
            "Epoch 1732/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8488 - val_loss: 2.3269\n",
            "Epoch 1733/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8352 - val_loss: 2.3684\n",
            "Epoch 1734/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8100 - val_loss: 2.2774\n",
            "Epoch 1735/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8376 - val_loss: 2.5286\n",
            "Epoch 1736/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8319 - val_loss: 2.3777\n",
            "Epoch 1737/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8105 - val_loss: 2.4336\n",
            "Epoch 1738/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8623 - val_loss: 2.3148\n",
            "Epoch 1739/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8059 - val_loss: 2.3307\n",
            "Epoch 1740/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8107 - val_loss: 2.3125\n",
            "Epoch 1741/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8624 - val_loss: 2.2796\n",
            "Epoch 1742/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8530 - val_loss: 2.2910\n",
            "Epoch 1743/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8229 - val_loss: 2.3823\n",
            "Epoch 1744/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7947 - val_loss: 2.3005\n",
            "Epoch 1745/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8054 - val_loss: 2.2891\n",
            "Epoch 1746/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7976 - val_loss: 2.3348\n",
            "Epoch 1747/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8248 - val_loss: 2.3540\n",
            "Epoch 1748/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8402 - val_loss: 2.3050\n",
            "Epoch 1749/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8223 - val_loss: 2.4416\n",
            "Epoch 1750/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8253 - val_loss: 2.4616\n",
            "Epoch 1751/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8312 - val_loss: 2.3294\n",
            "Epoch 1752/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7837 - val_loss: 2.3212\n",
            "Epoch 1753/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8637 - val_loss: 2.5158\n",
            "Epoch 1754/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8276 - val_loss: 2.3019\n",
            "Epoch 1755/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7821 - val_loss: 2.3539\n",
            "Epoch 1756/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8103 - val_loss: 2.2806\n",
            "Epoch 1757/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8454 - val_loss: 2.3315\n",
            "Epoch 1758/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8032 - val_loss: 2.3133\n",
            "Epoch 1759/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7995 - val_loss: 2.3865\n",
            "Epoch 1760/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8192 - val_loss: 2.3643\n",
            "Epoch 1761/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8300 - val_loss: 2.3713\n",
            "Epoch 1762/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8216 - val_loss: 2.3935\n",
            "Epoch 1763/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8100 - val_loss: 2.2821\n",
            "Epoch 1764/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8263 - val_loss: 2.2890\n",
            "Epoch 1765/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8317 - val_loss: 2.2750\n",
            "Epoch 1766/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8335 - val_loss: 2.3160\n",
            "Epoch 1767/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8110 - val_loss: 2.2693\n",
            "Epoch 1768/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7924 - val_loss: 2.3808\n",
            "Epoch 1769/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8059 - val_loss: 2.3279\n",
            "Epoch 1770/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7811 - val_loss: 2.2886\n",
            "Epoch 1771/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8058 - val_loss: 2.3087\n",
            "Epoch 1772/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8099 - val_loss: 2.3012\n",
            "Epoch 1773/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8107 - val_loss: 2.4515\n",
            "Epoch 1774/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8600 - val_loss: 2.3341\n",
            "Epoch 1775/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8133 - val_loss: 2.2761\n",
            "Epoch 1776/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8282 - val_loss: 2.3363\n",
            "Epoch 1777/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8046 - val_loss: 2.3298\n",
            "Epoch 1778/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7872 - val_loss: 2.2932\n",
            "Epoch 1779/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7931 - val_loss: 2.2734\n",
            "Epoch 1780/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7810 - val_loss: 2.3457\n",
            "Epoch 1781/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7935 - val_loss: 2.2730\n",
            "Epoch 1782/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7804 - val_loss: 2.3099\n",
            "Epoch 1783/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8588 - val_loss: 2.2942\n",
            "Epoch 1784/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8139 - val_loss: 2.5179\n",
            "Epoch 1785/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8291 - val_loss: 2.3033\n",
            "Epoch 1786/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8205 - val_loss: 2.4389\n",
            "Epoch 1787/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8204 - val_loss: 2.2946\n",
            "Epoch 1788/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7855 - val_loss: 2.3245\n",
            "Epoch 1789/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8375 - val_loss: 2.3035\n",
            "Epoch 1790/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8316 - val_loss: 2.2722\n",
            "Epoch 1791/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8067 - val_loss: 2.3603\n",
            "Epoch 1792/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7926 - val_loss: 2.2770\n",
            "Epoch 1793/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7823 - val_loss: 2.2913\n",
            "Epoch 1794/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8209 - val_loss: 2.3480\n",
            "Epoch 1795/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7975 - val_loss: 2.3151\n",
            "Epoch 1796/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8354 - val_loss: 2.4509\n",
            "Epoch 1797/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8112 - val_loss: 2.3329\n",
            "Epoch 1798/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8097 - val_loss: 2.3045\n",
            "Epoch 1799/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8082 - val_loss: 2.3465\n",
            "Epoch 1800/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8402 - val_loss: 2.3272\n",
            "Epoch 1801/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7694 - val_loss: 2.2773\n",
            "Epoch 1802/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8011 - val_loss: 2.3198\n",
            "Epoch 1803/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8731 - val_loss: 2.4203\n",
            "Epoch 1804/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8616 - val_loss: 2.3203\n",
            "Epoch 1805/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8058 - val_loss: 2.3743\n",
            "Epoch 1806/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8144 - val_loss: 2.2864\n",
            "Epoch 1807/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8244 - val_loss: 2.3647\n",
            "Epoch 1808/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7765 - val_loss: 2.3122\n",
            "Epoch 1809/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7751 - val_loss: 2.2985\n",
            "Epoch 1810/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7956 - val_loss: 2.3652\n",
            "Epoch 1811/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8177 - val_loss: 2.3006\n",
            "Epoch 1812/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7926 - val_loss: 2.3348\n",
            "Epoch 1813/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8220 - val_loss: 2.3781\n",
            "Epoch 1814/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8331 - val_loss: 2.4472\n",
            "Epoch 1815/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8029 - val_loss: 2.2718\n",
            "Epoch 1816/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7584 - val_loss: 2.4007\n",
            "Epoch 1817/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8130 - val_loss: 2.3598\n",
            "Epoch 1818/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8074 - val_loss: 2.3083\n",
            "Epoch 1819/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8045 - val_loss: 2.3115\n",
            "Epoch 1820/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7826 - val_loss: 2.2747\n",
            "Epoch 1821/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7831 - val_loss: 2.3746\n",
            "Epoch 1822/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8268 - val_loss: 2.2980\n",
            "Epoch 1823/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7838 - val_loss: 2.3873\n",
            "Epoch 1824/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7953 - val_loss: 2.2651\n",
            "Epoch 1825/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8846 - val_loss: 2.3238\n",
            "Epoch 1826/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7774 - val_loss: 2.3597\n",
            "Epoch 1827/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7790 - val_loss: 2.4147\n",
            "Epoch 1828/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8017 - val_loss: 2.3279\n",
            "Epoch 1829/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8115 - val_loss: 2.3962\n",
            "Epoch 1830/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7919 - val_loss: 2.3221\n",
            "Epoch 1831/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8537 - val_loss: 2.2986\n",
            "Epoch 1832/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7970 - val_loss: 2.3831\n",
            "Epoch 1833/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7765 - val_loss: 2.3483\n",
            "Epoch 1834/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8146 - val_loss: 2.3414\n",
            "Epoch 1835/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8566 - val_loss: 2.3027\n",
            "Epoch 1836/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7790 - val_loss: 2.3468\n",
            "Epoch 1837/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7984 - val_loss: 2.4499\n",
            "Epoch 1838/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8262 - val_loss: 2.3789\n",
            "Epoch 1839/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7552 - val_loss: 2.3248\n",
            "Epoch 1840/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7962 - val_loss: 2.2881\n",
            "Epoch 1841/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7828 - val_loss: 2.2659\n",
            "Epoch 1842/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7796 - val_loss: 2.2997\n",
            "Epoch 1843/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8172 - val_loss: 2.4426\n",
            "Epoch 1844/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8197 - val_loss: 2.3315\n",
            "Epoch 1845/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8051 - val_loss: 2.2965\n",
            "Epoch 1846/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7984 - val_loss: 2.2558\n",
            "Epoch 1847/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7698 - val_loss: 2.3434\n",
            "Epoch 1848/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7929 - val_loss: 2.3123\n",
            "Epoch 1849/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8151 - val_loss: 2.3496\n",
            "Epoch 1850/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8181 - val_loss: 2.2989\n",
            "Epoch 1851/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7778 - val_loss: 2.4129\n",
            "Epoch 1852/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7977 - val_loss: 2.2823\n",
            "Epoch 1853/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7770 - val_loss: 2.3183\n",
            "Epoch 1854/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7699 - val_loss: 2.2684\n",
            "Epoch 1855/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8277 - val_loss: 2.2756\n",
            "Epoch 1856/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7826 - val_loss: 2.2832\n",
            "Epoch 1857/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7774 - val_loss: 2.2668\n",
            "Epoch 1858/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8029 - val_loss: 2.4236\n",
            "Epoch 1859/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8288 - val_loss: 2.2952\n",
            "Epoch 1860/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8119 - val_loss: 2.3810\n",
            "Epoch 1861/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8077 - val_loss: 2.3261\n",
            "Epoch 1862/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7857 - val_loss: 2.2733\n",
            "Epoch 1863/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8005 - val_loss: 2.3709\n",
            "Epoch 1864/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8179 - val_loss: 2.3044\n",
            "Epoch 1865/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.8023 - val_loss: 2.3404\n",
            "Epoch 1866/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7998 - val_loss: 2.3175\n",
            "Epoch 1867/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7911 - val_loss: 2.3048\n",
            "Epoch 1868/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7959 - val_loss: 2.2900\n",
            "Epoch 1869/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7728 - val_loss: 2.4375\n",
            "Epoch 1870/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7792 - val_loss: 2.3236\n",
            "Epoch 1871/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8180 - val_loss: 2.2971\n",
            "Epoch 1872/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8178 - val_loss: 2.2644\n",
            "Epoch 1873/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7816 - val_loss: 2.3155\n",
            "Epoch 1874/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7914 - val_loss: 2.3071\n",
            "Epoch 1875/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7570 - val_loss: 2.3095\n",
            "Epoch 1876/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7777 - val_loss: 2.3035\n",
            "Epoch 1877/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8131 - val_loss: 2.3472\n",
            "Epoch 1878/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8336 - val_loss: 2.4086\n",
            "Epoch 1879/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7813 - val_loss: 2.3683\n",
            "Epoch 1880/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7971 - val_loss: 2.3694\n",
            "Epoch 1881/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7893 - val_loss: 2.3289\n",
            "Epoch 1882/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7820 - val_loss: 2.2916\n",
            "Epoch 1883/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8866 - val_loss: 2.2813\n",
            "Epoch 1884/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7967 - val_loss: 2.4338\n",
            "Epoch 1885/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7681 - val_loss: 2.3362\n",
            "Epoch 1886/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8016 - val_loss: 2.2927\n",
            "Epoch 1887/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7691 - val_loss: 2.3493\n",
            "Epoch 1888/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7917 - val_loss: 2.2928\n",
            "Epoch 1889/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7588 - val_loss: 2.3575\n",
            "Epoch 1890/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7738 - val_loss: 2.2812\n",
            "Epoch 1891/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7776 - val_loss: 2.3296\n",
            "Epoch 1892/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7982 - val_loss: 2.2951\n",
            "Epoch 1893/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7794 - val_loss: 2.3662\n",
            "Epoch 1894/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7765 - val_loss: 2.2898\n",
            "Epoch 1895/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7826 - val_loss: 2.3897\n",
            "Epoch 1896/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7759 - val_loss: 2.2711\n",
            "Epoch 1897/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7480 - val_loss: 2.2872\n",
            "Epoch 1898/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8051 - val_loss: 2.2754\n",
            "Epoch 1899/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7880 - val_loss: 2.3063\n",
            "Epoch 1900/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7715 - val_loss: 2.2764\n",
            "Epoch 1901/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8188 - val_loss: 2.3914\n",
            "Epoch 1902/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8020 - val_loss: 2.5261\n",
            "Epoch 1903/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8406 - val_loss: 2.3331\n",
            "Epoch 1904/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7725 - val_loss: 2.3181\n",
            "Epoch 1905/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7979 - val_loss: 2.4539\n",
            "Epoch 1906/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7963 - val_loss: 2.3261\n",
            "Epoch 1907/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7774 - val_loss: 2.3121\n",
            "Epoch 1908/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7634 - val_loss: 2.3127\n",
            "Epoch 1909/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7774 - val_loss: 2.3209\n",
            "Epoch 1910/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8172 - val_loss: 2.2595\n",
            "Epoch 1911/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8004 - val_loss: 2.2869\n",
            "Epoch 1912/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7755 - val_loss: 2.3375\n",
            "Epoch 1913/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7600 - val_loss: 2.3450\n",
            "Epoch 1914/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7795 - val_loss: 2.3526\n",
            "Epoch 1915/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8135 - val_loss: 2.3280\n",
            "Epoch 1916/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8133 - val_loss: 2.3176\n",
            "Epoch 1917/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7712 - val_loss: 2.3223\n",
            "Epoch 1918/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7538 - val_loss: 2.3427\n",
            "Epoch 1919/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7923 - val_loss: 2.4448\n",
            "Epoch 1920/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7958 - val_loss: 2.3234\n",
            "Epoch 1921/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7456 - val_loss: 2.3542\n",
            "Epoch 1922/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8043 - val_loss: 2.2862\n",
            "Epoch 1923/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8305 - val_loss: 2.3025\n",
            "Epoch 1924/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7572 - val_loss: 2.3049\n",
            "Epoch 1925/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7712 - val_loss: 2.2797\n",
            "Epoch 1926/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7981 - val_loss: 2.3162\n",
            "Epoch 1927/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7789 - val_loss: 2.3310\n",
            "Epoch 1928/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7531 - val_loss: 2.3581\n",
            "Epoch 1929/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7855 - val_loss: 2.3790\n",
            "Epoch 1930/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7923 - val_loss: 2.3594\n",
            "Epoch 1931/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8013 - val_loss: 2.2820\n",
            "Epoch 1932/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7794 - val_loss: 2.2872\n",
            "Epoch 1933/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7651 - val_loss: 2.3593\n",
            "Epoch 1934/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7703 - val_loss: 2.3122\n",
            "Epoch 1935/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7556 - val_loss: 2.3583\n",
            "Epoch 1936/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7672 - val_loss: 2.2810\n",
            "Epoch 1937/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7599 - val_loss: 2.5527\n",
            "Epoch 1938/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7776 - val_loss: 2.3009\n",
            "Epoch 1939/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8108 - val_loss: 2.3235\n",
            "Epoch 1940/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7766 - val_loss: 2.3459\n",
            "Epoch 1941/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7662 - val_loss: 2.3647\n",
            "Epoch 1942/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8056 - val_loss: 2.3404\n",
            "Epoch 1943/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7715 - val_loss: 2.2705\n",
            "Epoch 1944/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7901 - val_loss: 2.2710\n",
            "Epoch 1945/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7788 - val_loss: 2.3074\n",
            "Epoch 1946/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7539 - val_loss: 2.4349\n",
            "Epoch 1947/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7962 - val_loss: 2.3846\n",
            "Epoch 1948/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7824 - val_loss: 2.3636\n",
            "Epoch 1949/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7948 - val_loss: 2.4019\n",
            "Epoch 1950/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7708 - val_loss: 2.3660\n",
            "Epoch 1951/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7583 - val_loss: 2.2985\n",
            "Epoch 1952/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7765 - val_loss: 2.2825\n",
            "Epoch 1953/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7646 - val_loss: 2.4107\n",
            "Epoch 1954/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7965 - val_loss: 2.2995\n",
            "Epoch 1955/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7534 - val_loss: 2.2683\n",
            "Epoch 1956/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7782 - val_loss: 2.3330\n",
            "Epoch 1957/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7647 - val_loss: 2.6551\n",
            "Epoch 1958/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7958 - val_loss: 2.2840\n",
            "Epoch 1959/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7746 - val_loss: 2.3227\n",
            "Epoch 1960/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7450 - val_loss: 2.2798\n",
            "Epoch 1961/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7423 - val_loss: 2.4348\n",
            "Epoch 1962/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7691 - val_loss: 2.3320\n",
            "Epoch 1963/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7494 - val_loss: 2.3115\n",
            "Epoch 1964/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7789 - val_loss: 2.3059\n",
            "Epoch 1965/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7792 - val_loss: 2.3032\n",
            "Epoch 1966/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7651 - val_loss: 2.2919\n",
            "Epoch 1967/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7236 - val_loss: 2.3985\n",
            "Epoch 1968/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7755 - val_loss: 2.2984\n",
            "Epoch 1969/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8164 - val_loss: 2.2758\n",
            "Epoch 1970/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7558 - val_loss: 2.4051\n",
            "Epoch 1971/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7720 - val_loss: 2.3114\n",
            "Epoch 1972/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7636 - val_loss: 2.3655\n",
            "Epoch 1973/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7621 - val_loss: 2.3111\n",
            "Epoch 1974/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7744 - val_loss: 2.6777\n",
            "Epoch 1975/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7832 - val_loss: 2.3271\n",
            "Epoch 1976/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7519 - val_loss: 2.2900\n",
            "Epoch 1977/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7753 - val_loss: 2.4016\n",
            "Epoch 1978/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7715 - val_loss: 2.3180\n",
            "Epoch 1979/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7506 - val_loss: 2.4136\n",
            "Epoch 1980/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.8220 - val_loss: 2.4363\n",
            "Epoch 1981/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7895 - val_loss: 2.3719\n",
            "Epoch 1982/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7457 - val_loss: 2.3669\n",
            "Epoch 1983/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8056 - val_loss: 2.2864\n",
            "Epoch 1984/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7801 - val_loss: 2.2732\n",
            "Epoch 1985/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7432 - val_loss: 2.3951\n",
            "Epoch 1986/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8136 - val_loss: 2.2933\n",
            "Epoch 1987/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7375 - val_loss: 2.3286\n",
            "Epoch 1988/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7555 - val_loss: 2.3225\n",
            "Epoch 1989/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7644 - val_loss: 2.3484\n",
            "Epoch 1990/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7805 - val_loss: 2.3795\n",
            "Epoch 1991/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7655 - val_loss: 2.3544\n",
            "Epoch 1992/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7685 - val_loss: 2.2710\n",
            "Epoch 1993/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7885 - val_loss: 2.4070\n",
            "Epoch 1994/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7488 - val_loss: 2.3409\n",
            "Epoch 1995/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7797 - val_loss: 2.3340\n",
            "Epoch 1996/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7788 - val_loss: 2.2798\n",
            "Epoch 1997/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7605 - val_loss: 2.2935\n",
            "Epoch 1998/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7561 - val_loss: 2.2898\n",
            "Epoch 1999/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8042 - val_loss: 2.3443\n",
            "Epoch 2000/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.8315 - val_loss: 2.3036\n",
            "Epoch 2001/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7771 - val_loss: 2.3184\n",
            "Epoch 2002/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7806 - val_loss: 2.3676\n",
            "Epoch 2003/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7365 - val_loss: 2.3289\n",
            "Epoch 2004/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7733 - val_loss: 2.3053\n",
            "Epoch 2005/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7990 - val_loss: 2.3260\n",
            "Epoch 2006/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7502 - val_loss: 2.3012\n",
            "Epoch 2007/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7631 - val_loss: 2.3227\n",
            "Epoch 2008/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7637 - val_loss: 2.3587\n",
            "Epoch 2009/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7653 - val_loss: 2.3434\n",
            "Epoch 2010/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7398 - val_loss: 2.3069\n",
            "Epoch 2011/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7628 - val_loss: 2.3861\n",
            "Epoch 2012/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7394 - val_loss: 2.2683\n",
            "Epoch 2013/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7655 - val_loss: 2.2750\n",
            "Epoch 2014/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8192 - val_loss: 2.3177\n",
            "Epoch 2015/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7788 - val_loss: 2.3879\n",
            "Epoch 2016/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7570 - val_loss: 2.3351\n",
            "Epoch 2017/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7570 - val_loss: 2.3044\n",
            "Epoch 2018/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7330 - val_loss: 2.3509\n",
            "Epoch 2019/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7846 - val_loss: 2.3015\n",
            "Epoch 2020/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.8449 - val_loss: 2.3632\n",
            "Epoch 2021/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8029 - val_loss: 2.2641\n",
            "Epoch 2022/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7776 - val_loss: 2.4663\n",
            "Epoch 2023/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7297 - val_loss: 2.2741\n",
            "Epoch 2024/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7325 - val_loss: 2.3535\n",
            "Epoch 2025/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7416 - val_loss: 2.3372\n",
            "Epoch 2026/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8044 - val_loss: 2.3096\n",
            "Epoch 2027/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7242 - val_loss: 2.2858\n",
            "Epoch 2028/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7488 - val_loss: 2.3452\n",
            "Epoch 2029/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7771 - val_loss: 2.3647\n",
            "Epoch 2030/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7815 - val_loss: 2.2705\n",
            "Epoch 2031/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7315 - val_loss: 2.4918\n",
            "Epoch 2032/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7722 - val_loss: 2.3189\n",
            "Epoch 2033/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7728 - val_loss: 2.3866\n",
            "Epoch 2034/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7936 - val_loss: 2.3146\n",
            "Epoch 2035/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7178 - val_loss: 2.3389\n",
            "Epoch 2036/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7500 - val_loss: 2.3417\n",
            "Epoch 2037/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7477 - val_loss: 2.3731\n",
            "Epoch 2038/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7394 - val_loss: 2.3426\n",
            "Epoch 2039/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7526 - val_loss: 2.3148\n",
            "Epoch 2040/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7671 - val_loss: 2.3186\n",
            "Epoch 2041/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7737 - val_loss: 2.3664\n",
            "Epoch 2042/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7935 - val_loss: 2.3288\n",
            "Epoch 2043/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7578 - val_loss: 2.2812\n",
            "Epoch 2044/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7230 - val_loss: 2.3150\n",
            "Epoch 2045/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7415 - val_loss: 2.4865\n",
            "Epoch 2046/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.8163 - val_loss: 2.3804\n",
            "Epoch 2047/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7336 - val_loss: 2.2866\n",
            "Epoch 2048/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7230 - val_loss: 2.3334\n",
            "Epoch 2049/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7571 - val_loss: 2.2751\n",
            "Epoch 2050/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7655 - val_loss: 2.3216\n",
            "Epoch 2051/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7506 - val_loss: 2.2782\n",
            "Epoch 2052/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7456 - val_loss: 2.3263\n",
            "Epoch 2053/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7358 - val_loss: 2.3179\n",
            "Epoch 2054/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7475 - val_loss: 2.3504\n",
            "Epoch 2055/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8102 - val_loss: 2.4667\n",
            "Epoch 2056/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7639 - val_loss: 2.3105\n",
            "Epoch 2057/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7679 - val_loss: 2.2773\n",
            "Epoch 2058/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7264 - val_loss: 2.2632\n",
            "Epoch 2059/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7188 - val_loss: 2.3106\n",
            "Epoch 2060/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7610 - val_loss: 2.3132\n",
            "Epoch 2061/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7112 - val_loss: 2.3476\n",
            "Epoch 2062/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7719 - val_loss: 2.4078\n",
            "Epoch 2063/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7536 - val_loss: 2.3013\n",
            "Epoch 2064/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7377 - val_loss: 2.2857\n",
            "Epoch 2065/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7486 - val_loss: 2.2889\n",
            "Epoch 2066/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7490 - val_loss: 2.3835\n",
            "Epoch 2067/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7736 - val_loss: 2.3045\n",
            "Epoch 2068/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7671 - val_loss: 2.3628\n",
            "Epoch 2069/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7715 - val_loss: 2.3325\n",
            "Epoch 2070/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7601 - val_loss: 2.2998\n",
            "Epoch 2071/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7524 - val_loss: 2.3343\n",
            "Epoch 2072/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7359 - val_loss: 2.4331\n",
            "Epoch 2073/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7621 - val_loss: 2.3103\n",
            "Epoch 2074/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7563 - val_loss: 2.2948\n",
            "Epoch 2075/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7220 - val_loss: 2.2938\n",
            "Epoch 2076/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7387 - val_loss: 2.3027\n",
            "Epoch 2077/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7254 - val_loss: 2.3179\n",
            "Epoch 2078/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7209 - val_loss: 2.3885\n",
            "Epoch 2079/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7304 - val_loss: 2.2787\n",
            "Epoch 2080/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7612 - val_loss: 2.2794\n",
            "Epoch 2081/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7383 - val_loss: 2.2952\n",
            "Epoch 2082/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7200 - val_loss: 2.2865\n",
            "Epoch 2083/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7169 - val_loss: 2.3412\n",
            "Epoch 2084/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7813 - val_loss: 2.3172\n",
            "Epoch 2085/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7306 - val_loss: 2.3920\n",
            "Epoch 2086/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7236 - val_loss: 2.3141\n",
            "Epoch 2087/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7542 - val_loss: 2.2846\n",
            "Epoch 2088/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7003 - val_loss: 2.2939\n",
            "Epoch 2089/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7402 - val_loss: 2.8948\n",
            "Epoch 2090/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7832 - val_loss: 2.3303\n",
            "Epoch 2091/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7343 - val_loss: 2.3435\n",
            "Epoch 2092/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7446 - val_loss: 2.2888\n",
            "Epoch 2093/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7466 - val_loss: 2.3345\n",
            "Epoch 2094/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7471 - val_loss: 2.5145\n",
            "Epoch 2095/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7231 - val_loss: 2.3274\n",
            "Epoch 2096/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7567 - val_loss: 2.3685\n",
            "Epoch 2097/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.8270 - val_loss: 2.3381\n",
            "Epoch 2098/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7222 - val_loss: 2.3144\n",
            "Epoch 2099/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7214 - val_loss: 2.2913\n",
            "Epoch 2100/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7103 - val_loss: 2.3335\n",
            "Epoch 2101/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7625 - val_loss: 2.2925\n",
            "Epoch 2102/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7332 - val_loss: 2.4036\n",
            "Epoch 2103/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7296 - val_loss: 2.3629\n",
            "Epoch 2104/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7717 - val_loss: 2.2990\n",
            "Epoch 2105/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7293 - val_loss: 2.3287\n",
            "Epoch 2106/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7546 - val_loss: 2.2967\n",
            "Epoch 2107/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7512 - val_loss: 2.2865\n",
            "Epoch 2108/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8022 - val_loss: 2.3344\n",
            "Epoch 2109/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7364 - val_loss: 2.4749\n",
            "Epoch 2110/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7535 - val_loss: 2.3380\n",
            "Epoch 2111/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7587 - val_loss: 2.3086\n",
            "Epoch 2112/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7282 - val_loss: 2.3229\n",
            "Epoch 2113/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7469 - val_loss: 2.2656\n",
            "Epoch 2114/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7349 - val_loss: 2.3493\n",
            "Epoch 2115/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7325 - val_loss: 2.3493\n",
            "Epoch 2116/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7248 - val_loss: 2.3841\n",
            "Epoch 2117/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7421 - val_loss: 2.3203\n",
            "Epoch 2118/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7345 - val_loss: 2.3706\n",
            "Epoch 2119/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.8037 - val_loss: 2.2815\n",
            "Epoch 2120/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7238 - val_loss: 2.2867\n",
            "Epoch 2121/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7142 - val_loss: 2.2835\n",
            "Epoch 2122/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7074 - val_loss: 2.2865\n",
            "Epoch 2123/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7136 - val_loss: 2.3061\n",
            "Epoch 2124/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7316 - val_loss: 2.3281\n",
            "Epoch 2125/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7409 - val_loss: 2.3209\n",
            "Epoch 2126/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7580 - val_loss: 2.2884\n",
            "Epoch 2127/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7329 - val_loss: 2.2789\n",
            "Epoch 2128/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6980 - val_loss: 2.3662\n",
            "Epoch 2129/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7784 - val_loss: 2.3964\n",
            "Epoch 2130/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.8156 - val_loss: 2.4229\n",
            "Epoch 2131/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7191 - val_loss: 2.3000\n",
            "Epoch 2132/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7037 - val_loss: 2.2884\n",
            "Epoch 2133/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7089 - val_loss: 2.3286\n",
            "Epoch 2134/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7038 - val_loss: 2.2989\n",
            "Epoch 2135/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7118 - val_loss: 2.2836\n",
            "Epoch 2136/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7248 - val_loss: 2.2779\n",
            "Epoch 2137/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7568 - val_loss: 2.3231\n",
            "Epoch 2138/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7697 - val_loss: 2.3589\n",
            "Epoch 2139/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7644 - val_loss: 2.3468\n",
            "Epoch 2140/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7048 - val_loss: 2.4645\n",
            "Epoch 2141/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7635 - val_loss: 2.3438\n",
            "Epoch 2142/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.7384 - val_loss: 2.3411\n",
            "Epoch 2143/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7176 - val_loss: 2.3100\n",
            "Epoch 2144/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7352 - val_loss: 2.2944\n",
            "Epoch 2145/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7513 - val_loss: 2.3407\n",
            "Epoch 2146/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7233 - val_loss: 2.4165\n",
            "Epoch 2147/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7320 - val_loss: 2.3188\n",
            "Epoch 2148/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7453 - val_loss: 2.3409\n",
            "Epoch 2149/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7299 - val_loss: 2.3585\n",
            "Epoch 2150/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7127 - val_loss: 2.3099\n",
            "Epoch 2151/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7064 - val_loss: 2.3143\n",
            "Epoch 2152/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7675 - val_loss: 2.2803\n",
            "Epoch 2153/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7228 - val_loss: 2.3761\n",
            "Epoch 2154/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7188 - val_loss: 2.2925\n",
            "Epoch 2155/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6968 - val_loss: 2.2966\n",
            "Epoch 2156/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7317 - val_loss: 2.4339\n",
            "Epoch 2157/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7105 - val_loss: 2.2735\n",
            "Epoch 2158/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7170 - val_loss: 2.3577\n",
            "Epoch 2159/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7566 - val_loss: 2.3892\n",
            "Epoch 2160/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7568 - val_loss: 2.3953\n",
            "Epoch 2161/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7479 - val_loss: 2.2931\n",
            "Epoch 2162/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7295 - val_loss: 2.3414\n",
            "Epoch 2163/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7282 - val_loss: 2.3374\n",
            "Epoch 2164/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7409 - val_loss: 2.3408\n",
            "Epoch 2165/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7806 - val_loss: 2.2870\n",
            "Epoch 2166/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7212 - val_loss: 2.3276\n",
            "Epoch 2167/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7309 - val_loss: 2.3968\n",
            "Epoch 2168/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7257 - val_loss: 2.3516\n",
            "Epoch 2169/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7232 - val_loss: 2.3593\n",
            "Epoch 2170/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6979 - val_loss: 2.3718\n",
            "Epoch 2171/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7131 - val_loss: 2.3123\n",
            "Epoch 2172/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6885 - val_loss: 2.2757\n",
            "Epoch 2173/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7537 - val_loss: 2.4506\n",
            "Epoch 2174/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7396 - val_loss: 2.3222\n",
            "Epoch 2175/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7223 - val_loss: 2.3192\n",
            "Epoch 2176/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7097 - val_loss: 2.3541\n",
            "Epoch 2177/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7083 - val_loss: 2.2911\n",
            "Epoch 2178/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7198 - val_loss: 2.3008\n",
            "Epoch 2179/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7022 - val_loss: 2.4463\n",
            "Epoch 2180/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7622 - val_loss: 2.4517\n",
            "Epoch 2181/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7763 - val_loss: 2.4638\n",
            "Epoch 2182/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7428 - val_loss: 2.3228\n",
            "Epoch 2183/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6987 - val_loss: 2.3085\n",
            "Epoch 2184/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7286 - val_loss: 2.3358\n",
            "Epoch 2185/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7342 - val_loss: 2.3398\n",
            "Epoch 2186/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6963 - val_loss: 2.3110\n",
            "Epoch 2187/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7381 - val_loss: 2.3366\n",
            "Epoch 2188/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7044 - val_loss: 2.3006\n",
            "Epoch 2189/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6861 - val_loss: 2.2754\n",
            "Epoch 2190/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7604 - val_loss: 2.3628\n",
            "Epoch 2191/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7393 - val_loss: 2.2761\n",
            "Epoch 2192/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7188 - val_loss: 2.3098\n",
            "Epoch 2193/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7461 - val_loss: 2.3047\n",
            "Epoch 2194/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6896 - val_loss: 2.3127\n",
            "Epoch 2195/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7427 - val_loss: 2.4629\n",
            "Epoch 2196/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7480 - val_loss: 2.2944\n",
            "Epoch 2197/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7124 - val_loss: 2.3536\n",
            "Epoch 2198/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7851 - val_loss: 2.4162\n",
            "Epoch 2199/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7226 - val_loss: 2.2740\n",
            "Epoch 2200/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7094 - val_loss: 2.3171\n",
            "Epoch 2201/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6794 - val_loss: 2.2933\n",
            "Epoch 2202/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7002 - val_loss: 2.3107\n",
            "Epoch 2203/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7141 - val_loss: 2.3160\n",
            "Epoch 2204/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7384 - val_loss: 2.3058\n",
            "Epoch 2205/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7172 - val_loss: 2.3211\n",
            "Epoch 2206/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7541 - val_loss: 2.3376\n",
            "Epoch 2207/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7275 - val_loss: 2.4604\n",
            "Epoch 2208/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7601 - val_loss: 2.3045\n",
            "Epoch 2209/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7312 - val_loss: 2.2824\n",
            "Epoch 2210/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7084 - val_loss: 2.3160\n",
            "Epoch 2211/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7522 - val_loss: 2.2992\n",
            "Epoch 2212/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6972 - val_loss: 2.2915\n",
            "Epoch 2213/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6790 - val_loss: 2.3328\n",
            "Epoch 2214/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7231 - val_loss: 2.3354\n",
            "Epoch 2215/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7175 - val_loss: 2.3364\n",
            "Epoch 2216/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7404 - val_loss: 2.3065\n",
            "Epoch 2217/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7381 - val_loss: 2.3085\n",
            "Epoch 2218/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7213 - val_loss: 2.2933\n",
            "Epoch 2219/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7031 - val_loss: 2.3428\n",
            "Epoch 2220/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7432 - val_loss: 2.3418\n",
            "Epoch 2221/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7242 - val_loss: 2.3243\n",
            "Epoch 2222/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7313 - val_loss: 2.3110\n",
            "Epoch 2223/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7476 - val_loss: 2.3546\n",
            "Epoch 2224/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7185 - val_loss: 2.3052\n",
            "Epoch 2225/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7565 - val_loss: 2.4498\n",
            "Epoch 2226/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7058 - val_loss: 2.3226\n",
            "Epoch 2227/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6989 - val_loss: 2.2761\n",
            "Epoch 2228/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7014 - val_loss: 2.2888\n",
            "Epoch 2229/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7193 - val_loss: 2.3416\n",
            "Epoch 2230/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7436 - val_loss: 2.3526\n",
            "Epoch 2231/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6994 - val_loss: 2.4004\n",
            "Epoch 2232/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7101 - val_loss: 2.3697\n",
            "Epoch 2233/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7066 - val_loss: 2.3431\n",
            "Epoch 2234/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7180 - val_loss: 2.3593\n",
            "Epoch 2235/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7342 - val_loss: 2.3178\n",
            "Epoch 2236/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7241 - val_loss: 2.2842\n",
            "Epoch 2237/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7310 - val_loss: 2.3044\n",
            "Epoch 2238/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6920 - val_loss: 2.3589\n",
            "Epoch 2239/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7079 - val_loss: 2.2931\n",
            "Epoch 2240/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6983 - val_loss: 2.3107\n",
            "Epoch 2241/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7109 - val_loss: 2.3294\n",
            "Epoch 2242/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6999 - val_loss: 2.3119\n",
            "Epoch 2243/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7058 - val_loss: 2.3139\n",
            "Epoch 2244/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7154 - val_loss: 2.3496\n",
            "Epoch 2245/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7001 - val_loss: 2.3346\n",
            "Epoch 2246/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7043 - val_loss: 2.2934\n",
            "Epoch 2247/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7158 - val_loss: 2.3213\n",
            "Epoch 2248/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7658 - val_loss: 2.3058\n",
            "Epoch 2249/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7365 - val_loss: 2.2847\n",
            "Epoch 2250/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7451 - val_loss: 2.3473\n",
            "Epoch 2251/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7129 - val_loss: 2.3095\n",
            "Epoch 2252/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7031 - val_loss: 2.4690\n",
            "Epoch 2253/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7280 - val_loss: 2.3212\n",
            "Epoch 2254/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7312 - val_loss: 2.5739\n",
            "Epoch 2255/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7493 - val_loss: 2.3573\n",
            "Epoch 2256/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7142 - val_loss: 2.3396\n",
            "Epoch 2257/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6982 - val_loss: 2.2986\n",
            "Epoch 2258/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7375 - val_loss: 2.3189\n",
            "Epoch 2259/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6962 - val_loss: 2.2726\n",
            "Epoch 2260/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6820 - val_loss: 2.3836\n",
            "Epoch 2261/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7428 - val_loss: 2.4737\n",
            "Epoch 2262/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7473 - val_loss: 2.3351\n",
            "Epoch 2263/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7034 - val_loss: 2.3145\n",
            "Epoch 2264/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6835 - val_loss: 2.3605\n",
            "Epoch 2265/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6849 - val_loss: 2.3733\n",
            "Epoch 2266/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7000 - val_loss: 2.2924\n",
            "Epoch 2267/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6976 - val_loss: 2.3535\n",
            "Epoch 2268/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7064 - val_loss: 2.2993\n",
            "Epoch 2269/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6914 - val_loss: 2.2815\n",
            "Epoch 2270/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7035 - val_loss: 2.3571\n",
            "Epoch 2271/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7025 - val_loss: 2.2941\n",
            "Epoch 2272/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7210 - val_loss: 2.4057\n",
            "Epoch 2273/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7514 - val_loss: 2.3099\n",
            "Epoch 2274/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7140 - val_loss: 2.3602\n",
            "Epoch 2275/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7022 - val_loss: 2.2831\n",
            "Epoch 2276/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7091 - val_loss: 2.2758\n",
            "Epoch 2277/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7280 - val_loss: 2.3580\n",
            "Epoch 2278/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7610 - val_loss: 2.3540\n",
            "Epoch 2279/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7293 - val_loss: 2.2942\n",
            "Epoch 2280/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7008 - val_loss: 2.3326\n",
            "Epoch 2281/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6840 - val_loss: 2.2828\n",
            "Epoch 2282/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7335 - val_loss: 2.3599\n",
            "Epoch 2283/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7331 - val_loss: 2.3628\n",
            "Epoch 2284/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6922 - val_loss: 2.2918\n",
            "Epoch 2285/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6909 - val_loss: 2.3258\n",
            "Epoch 2286/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6890 - val_loss: 2.3338\n",
            "Epoch 2287/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7084 - val_loss: 2.3348\n",
            "Epoch 2288/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7015 - val_loss: 2.4169\n",
            "Epoch 2289/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6998 - val_loss: 2.2987\n",
            "Epoch 2290/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6877 - val_loss: 2.3188\n",
            "Epoch 2291/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7329 - val_loss: 2.3012\n",
            "Epoch 2292/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7178 - val_loss: 2.2958\n",
            "Epoch 2293/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7060 - val_loss: 2.5419\n",
            "Epoch 2294/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7642 - val_loss: 2.2981\n",
            "Epoch 2295/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6836 - val_loss: 2.2961\n",
            "Epoch 2296/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6858 - val_loss: 2.3248\n",
            "Epoch 2297/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7102 - val_loss: 2.3155\n",
            "Epoch 2298/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6888 - val_loss: 2.3519\n",
            "Epoch 2299/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7333 - val_loss: 2.2999\n",
            "Epoch 2300/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6859 - val_loss: 2.3795\n",
            "Epoch 2301/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7208 - val_loss: 2.2924\n",
            "Epoch 2302/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6741 - val_loss: 2.4110\n",
            "Epoch 2303/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7244 - val_loss: 2.3185\n",
            "Epoch 2304/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7789 - val_loss: 2.3862\n",
            "Epoch 2305/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7233 - val_loss: 2.3152\n",
            "Epoch 2306/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6720 - val_loss: 2.2925\n",
            "Epoch 2307/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7286 - val_loss: 2.4542\n",
            "Epoch 2308/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6900 - val_loss: 2.2757\n",
            "Epoch 2309/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6841 - val_loss: 2.2897\n",
            "Epoch 2310/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7026 - val_loss: 2.2886\n",
            "Epoch 2311/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7243 - val_loss: 2.3415\n",
            "Epoch 2312/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6863 - val_loss: 2.3702\n",
            "Epoch 2313/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6865 - val_loss: 2.2981\n",
            "Epoch 2314/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7060 - val_loss: 2.3087\n",
            "Epoch 2315/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6907 - val_loss: 2.3107\n",
            "Epoch 2316/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6771 - val_loss: 2.2995\n",
            "Epoch 2317/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7082 - val_loss: 2.3128\n",
            "Epoch 2318/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6820 - val_loss: 2.4246\n",
            "Epoch 2319/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7512 - val_loss: 2.3073\n",
            "Epoch 2320/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6954 - val_loss: 2.3638\n",
            "Epoch 2321/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7276 - val_loss: 2.2958\n",
            "Epoch 2322/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6701 - val_loss: 2.2946\n",
            "Epoch 2323/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7329 - val_loss: 2.2888\n",
            "Epoch 2324/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6995 - val_loss: 2.2960\n",
            "Epoch 2325/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7331 - val_loss: 2.3173\n",
            "Epoch 2326/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6725 - val_loss: 2.2831\n",
            "Epoch 2327/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6770 - val_loss: 2.3004\n",
            "Epoch 2328/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6647 - val_loss: 2.3722\n",
            "Epoch 2329/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6848 - val_loss: 2.2994\n",
            "Epoch 2330/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6968 - val_loss: 2.2646\n",
            "Epoch 2331/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6602 - val_loss: 2.3304\n",
            "Epoch 2332/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7198 - val_loss: 2.3040\n",
            "Epoch 2333/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6728 - val_loss: 2.3152\n",
            "Epoch 2334/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6658 - val_loss: 2.2997\n",
            "Epoch 2335/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6794 - val_loss: 2.3090\n",
            "Epoch 2336/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6853 - val_loss: 2.3431\n",
            "Epoch 2337/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6881 - val_loss: 2.3399\n",
            "Epoch 2338/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6712 - val_loss: 2.3301\n",
            "Epoch 2339/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7294 - val_loss: 2.2917\n",
            "Epoch 2340/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6986 - val_loss: 2.3305\n",
            "Epoch 2341/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7075 - val_loss: 2.3180\n",
            "Epoch 2342/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6750 - val_loss: 2.3058\n",
            "Epoch 2343/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6661 - val_loss: 2.4352\n",
            "Epoch 2344/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6888 - val_loss: 2.2914\n",
            "Epoch 2345/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7110 - val_loss: 2.3198\n",
            "Epoch 2346/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6900 - val_loss: 2.3053\n",
            "Epoch 2347/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6814 - val_loss: 2.3454\n",
            "Epoch 2348/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6980 - val_loss: 2.2841\n",
            "Epoch 2349/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6794 - val_loss: 2.3386\n",
            "Epoch 2350/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6557 - val_loss: 2.2919\n",
            "Epoch 2351/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6723 - val_loss: 2.3359\n",
            "Epoch 2352/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7633 - val_loss: 2.2779\n",
            "Epoch 2353/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7225 - val_loss: 2.3512\n",
            "Epoch 2354/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7316 - val_loss: 2.3552\n",
            "Epoch 2355/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7514 - val_loss: 2.2976\n",
            "Epoch 2356/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6540 - val_loss: 2.2711\n",
            "Epoch 2357/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7225 - val_loss: 2.2971\n",
            "Epoch 2358/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6807 - val_loss: 2.3417\n",
            "Epoch 2359/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6952 - val_loss: 2.2493\n",
            "Epoch 2360/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7278 - val_loss: 2.4055\n",
            "Epoch 2361/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6776 - val_loss: 2.3199\n",
            "Epoch 2362/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6940 - val_loss: 2.3171\n",
            "Epoch 2363/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6765 - val_loss: 2.3084\n",
            "Epoch 2364/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6770 - val_loss: 2.3881\n",
            "Epoch 2365/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6605 - val_loss: 2.3224\n",
            "Epoch 2366/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6775 - val_loss: 2.3301\n",
            "Epoch 2367/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7015 - val_loss: 2.3164\n",
            "Epoch 2368/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6841 - val_loss: 2.3458\n",
            "Epoch 2369/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6695 - val_loss: 2.2866\n",
            "Epoch 2370/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6770 - val_loss: 2.3394\n",
            "Epoch 2371/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7030 - val_loss: 2.4039\n",
            "Epoch 2372/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7577 - val_loss: 2.4047\n",
            "Epoch 2373/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6727 - val_loss: 2.2808\n",
            "Epoch 2374/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6965 - val_loss: 2.3143\n",
            "Epoch 2375/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6861 - val_loss: 2.3076\n",
            "Epoch 2376/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7007 - val_loss: 2.3486\n",
            "Epoch 2377/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7268 - val_loss: 2.4317\n",
            "Epoch 2378/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6763 - val_loss: 2.2892\n",
            "Epoch 2379/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7491 - val_loss: 2.3059\n",
            "Epoch 2380/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6755 - val_loss: 2.3612\n",
            "Epoch 2381/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6826 - val_loss: 2.3095\n",
            "Epoch 2382/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6880 - val_loss: 2.3327\n",
            "Epoch 2383/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7129 - val_loss: 2.2951\n",
            "Epoch 2384/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6685 - val_loss: 2.2891\n",
            "Epoch 2385/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6770 - val_loss: 2.2955\n",
            "Epoch 2386/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6823 - val_loss: 2.3012\n",
            "Epoch 2387/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6781 - val_loss: 2.3159\n",
            "Epoch 2388/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6727 - val_loss: 2.3265\n",
            "Epoch 2389/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6829 - val_loss: 2.3256\n",
            "Epoch 2390/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6849 - val_loss: 2.4431\n",
            "Epoch 2391/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.7016 - val_loss: 2.3116\n",
            "Epoch 2392/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7050 - val_loss: 2.2992\n",
            "Epoch 2393/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7275 - val_loss: 2.3673\n",
            "Epoch 2394/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6709 - val_loss: 2.3310\n",
            "Epoch 2395/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7162 - val_loss: 2.3577\n",
            "Epoch 2396/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6923 - val_loss: 2.3997\n",
            "Epoch 2397/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7077 - val_loss: 2.3142\n",
            "Epoch 2398/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6778 - val_loss: 2.3441\n",
            "Epoch 2399/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6693 - val_loss: 2.2827\n",
            "Epoch 2400/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7018 - val_loss: 2.2974\n",
            "Epoch 2401/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6833 - val_loss: 2.3333\n",
            "Epoch 2402/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6894 - val_loss: 2.2782\n",
            "Epoch 2403/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6764 - val_loss: 2.3380\n",
            "Epoch 2404/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6636 - val_loss: 2.3061\n",
            "Epoch 2405/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6843 - val_loss: 2.3440\n",
            "Epoch 2406/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7294 - val_loss: 2.3119\n",
            "Epoch 2407/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6757 - val_loss: 2.3326\n",
            "Epoch 2408/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7233 - val_loss: 2.3152\n",
            "Epoch 2409/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6587 - val_loss: 2.2760\n",
            "Epoch 2410/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6862 - val_loss: 2.3772\n",
            "Epoch 2411/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6675 - val_loss: 2.2855\n",
            "Epoch 2412/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6646 - val_loss: 2.3574\n",
            "Epoch 2413/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6693 - val_loss: 2.3117\n",
            "Epoch 2414/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6653 - val_loss: 2.3806\n",
            "Epoch 2415/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7200 - val_loss: 2.4909\n",
            "Epoch 2416/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6998 - val_loss: 2.3852\n",
            "Epoch 2417/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6499 - val_loss: 2.6617\n",
            "Epoch 2418/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7439 - val_loss: 2.2716\n",
            "Epoch 2419/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6720 - val_loss: 2.3295\n",
            "Epoch 2420/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6970 - val_loss: 2.2910\n",
            "Epoch 2421/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6536 - val_loss: 2.2831\n",
            "Epoch 2422/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6495 - val_loss: 2.3380\n",
            "Epoch 2423/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6878 - val_loss: 2.4809\n",
            "Epoch 2424/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6723 - val_loss: 2.3515\n",
            "Epoch 2425/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7177 - val_loss: 2.2698\n",
            "Epoch 2426/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6566 - val_loss: 2.2983\n",
            "Epoch 2427/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6416 - val_loss: 2.3118\n",
            "Epoch 2428/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6878 - val_loss: 2.3575\n",
            "Epoch 2429/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6667 - val_loss: 2.3140\n",
            "Epoch 2430/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6571 - val_loss: 2.4332\n",
            "Epoch 2431/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7268 - val_loss: 2.3033\n",
            "Epoch 2432/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6364 - val_loss: 2.3370\n",
            "Epoch 2433/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6728 - val_loss: 2.3185\n",
            "Epoch 2434/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.7271 - val_loss: 2.3203\n",
            "Epoch 2435/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7110 - val_loss: 2.3273\n",
            "Epoch 2436/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6862 - val_loss: 2.3045\n",
            "Epoch 2437/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6457 - val_loss: 2.2886\n",
            "Epoch 2438/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6444 - val_loss: 2.4078\n",
            "Epoch 2439/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6912 - val_loss: 2.3053\n",
            "Epoch 2440/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6866 - val_loss: 2.3116\n",
            "Epoch 2441/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6578 - val_loss: 2.3361\n",
            "Epoch 2442/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6409 - val_loss: 2.3022\n",
            "Epoch 2443/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6910 - val_loss: 2.3683\n",
            "Epoch 2444/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6731 - val_loss: 2.3193\n",
            "Epoch 2445/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6947 - val_loss: 2.3058\n",
            "Epoch 2446/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.6923 - val_loss: 2.3536\n",
            "Epoch 2447/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6667 - val_loss: 2.3740\n",
            "Epoch 2448/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6426 - val_loss: 2.3557\n",
            "Epoch 2449/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6841 - val_loss: 2.3059\n",
            "Epoch 2450/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6889 - val_loss: 2.3929\n",
            "Epoch 2451/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7091 - val_loss: 2.3815\n",
            "Epoch 2452/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6835 - val_loss: 2.3627\n",
            "Epoch 2453/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6726 - val_loss: 2.3103\n",
            "Epoch 2454/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6617 - val_loss: 2.3109\n",
            "Epoch 2455/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6741 - val_loss: 2.3321\n",
            "Epoch 2456/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6582 - val_loss: 2.2854\n",
            "Epoch 2457/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6529 - val_loss: 2.2946\n",
            "Epoch 2458/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6813 - val_loss: 2.3368\n",
            "Epoch 2459/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6612 - val_loss: 2.2902\n",
            "Epoch 2460/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6620 - val_loss: 2.3452\n",
            "Epoch 2461/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7213 - val_loss: 2.3698\n",
            "Epoch 2462/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6709 - val_loss: 2.4182\n",
            "Epoch 2463/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7571 - val_loss: 2.3594\n",
            "Epoch 2464/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6667 - val_loss: 2.3503\n",
            "Epoch 2465/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6464 - val_loss: 2.3012\n",
            "Epoch 2466/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6481 - val_loss: 2.2614\n",
            "Epoch 2467/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6497 - val_loss: 2.3804\n",
            "Epoch 2468/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6661 - val_loss: 2.3186\n",
            "Epoch 2469/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6581 - val_loss: 2.2842\n",
            "Epoch 2470/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6733 - val_loss: 2.2894\n",
            "Epoch 2471/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7250 - val_loss: 2.2830\n",
            "Epoch 2472/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6616 - val_loss: 2.2795\n",
            "Epoch 2473/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6656 - val_loss: 2.3399\n",
            "Epoch 2474/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6553 - val_loss: 2.3009\n",
            "Epoch 2475/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6545 - val_loss: 2.3332\n",
            "Epoch 2476/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6601 - val_loss: 2.3251\n",
            "Epoch 2477/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6716 - val_loss: 2.3341\n",
            "Epoch 2478/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6423 - val_loss: 2.3414\n",
            "Epoch 2479/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6748 - val_loss: 2.3156\n",
            "Epoch 2480/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6376 - val_loss: 2.3302\n",
            "Epoch 2481/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6679 - val_loss: 2.2905\n",
            "Epoch 2482/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6669 - val_loss: 2.2929\n",
            "Epoch 2483/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6470 - val_loss: 2.3115\n",
            "Epoch 2484/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.7093 - val_loss: 2.3264\n",
            "Epoch 2485/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6640 - val_loss: 2.2736\n",
            "Epoch 2486/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6650 - val_loss: 2.3412\n",
            "Epoch 2487/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6925 - val_loss: 2.3141\n",
            "Epoch 2488/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6770 - val_loss: 2.3107\n",
            "Epoch 2489/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6999 - val_loss: 2.2871\n",
            "Epoch 2490/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6546 - val_loss: 2.3306\n",
            "Epoch 2491/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6676 - val_loss: 2.2728\n",
            "Epoch 2492/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6434 - val_loss: 2.3624\n",
            "Epoch 2493/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6919 - val_loss: 2.3014\n",
            "Epoch 2494/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6360 - val_loss: 2.3314\n",
            "Epoch 2495/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6534 - val_loss: 2.3418\n",
            "Epoch 2496/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6636 - val_loss: 2.3734\n",
            "Epoch 2497/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6645 - val_loss: 2.2960\n",
            "Epoch 2498/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6956 - val_loss: 2.2936\n",
            "Epoch 2499/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6598 - val_loss: 2.3831\n",
            "Epoch 2500/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6792 - val_loss: 2.3040\n",
            "Epoch 2501/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6993 - val_loss: 2.2965\n",
            "Epoch 2502/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6921 - val_loss: 2.3111\n",
            "Epoch 2503/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6657 - val_loss: 2.3508\n",
            "Epoch 2504/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6792 - val_loss: 2.3652\n",
            "Epoch 2505/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6730 - val_loss: 2.2846\n",
            "Epoch 2506/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6713 - val_loss: 2.3704\n",
            "Epoch 2507/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6338 - val_loss: 2.3182\n",
            "Epoch 2508/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6624 - val_loss: 2.2764\n",
            "Epoch 2509/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6511 - val_loss: 2.3307\n",
            "Epoch 2510/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6717 - val_loss: 2.2918\n",
            "Epoch 2511/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6667 - val_loss: 2.3126\n",
            "Epoch 2512/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6576 - val_loss: 2.3156\n",
            "Epoch 2513/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6865 - val_loss: 2.3147\n",
            "Epoch 2514/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6704 - val_loss: 2.3622\n",
            "Epoch 2515/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6492 - val_loss: 2.3332\n",
            "Epoch 2516/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6599 - val_loss: 2.3012\n",
            "Epoch 2517/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6610 - val_loss: 2.3801\n",
            "Epoch 2518/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6547 - val_loss: 2.2990\n",
            "Epoch 2519/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6545 - val_loss: 2.3158\n",
            "Epoch 2520/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6643 - val_loss: 2.2967\n",
            "Epoch 2521/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7013 - val_loss: 2.2998\n",
            "Epoch 2522/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6546 - val_loss: 2.2885\n",
            "Epoch 2523/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6455 - val_loss: 2.3267\n",
            "Epoch 2524/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6482 - val_loss: 2.3001\n",
            "Epoch 2525/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6781 - val_loss: 2.3068\n",
            "Epoch 2526/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6543 - val_loss: 2.3164\n",
            "Epoch 2527/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6683 - val_loss: 2.3176\n",
            "Epoch 2528/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6598 - val_loss: 2.3518\n",
            "Epoch 2529/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6533 - val_loss: 2.3056\n",
            "Epoch 2530/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6459 - val_loss: 2.3053\n",
            "Epoch 2531/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6504 - val_loss: 2.2872\n",
            "Epoch 2532/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6528 - val_loss: 2.4594\n",
            "Epoch 2533/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6705 - val_loss: 2.3516\n",
            "Epoch 2534/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7078 - val_loss: 2.3222\n",
            "Epoch 2535/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6894 - val_loss: 2.2759\n",
            "Epoch 2536/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6705 - val_loss: 2.3305\n",
            "Epoch 2537/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6432 - val_loss: 2.3031\n",
            "Epoch 2538/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6424 - val_loss: 2.3090\n",
            "Epoch 2539/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6888 - val_loss: 2.6132\n",
            "Epoch 2540/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6731 - val_loss: 2.3589\n",
            "Epoch 2541/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7235 - val_loss: 2.3075\n",
            "Epoch 2542/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6459 - val_loss: 2.3520\n",
            "Epoch 2543/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6961 - val_loss: 2.3129\n",
            "Epoch 2544/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6565 - val_loss: 2.3216\n",
            "Epoch 2545/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6188 - val_loss: 2.2911\n",
            "Epoch 2546/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6677 - val_loss: 2.2995\n",
            "Epoch 2547/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6274 - val_loss: 2.2994\n",
            "Epoch 2548/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6413 - val_loss: 2.3487\n",
            "Epoch 2549/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6785 - val_loss: 2.3427\n",
            "Epoch 2550/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6827 - val_loss: 2.3552\n",
            "Epoch 2551/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6518 - val_loss: 2.2980\n",
            "Epoch 2552/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6344 - val_loss: 2.3401\n",
            "Epoch 2553/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6649 - val_loss: 2.2917\n",
            "Epoch 2554/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6647 - val_loss: 2.2758\n",
            "Epoch 2555/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6445 - val_loss: 2.3151\n",
            "Epoch 2556/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6812 - val_loss: 2.3544\n",
            "Epoch 2557/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6340 - val_loss: 2.3499\n",
            "Epoch 2558/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6188 - val_loss: 2.3576\n",
            "Epoch 2559/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6371 - val_loss: 2.3048\n",
            "Epoch 2560/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6198 - val_loss: 2.3199\n",
            "Epoch 2561/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6547 - val_loss: 2.3337\n",
            "Epoch 2562/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6393 - val_loss: 2.3614\n",
            "Epoch 2563/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7120 - val_loss: 2.4198\n",
            "Epoch 2564/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6789 - val_loss: 2.3408\n",
            "Epoch 2565/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.7240 - val_loss: 2.2940\n",
            "Epoch 2566/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6309 - val_loss: 2.3274\n",
            "Epoch 2567/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6450 - val_loss: 2.3207\n",
            "Epoch 2568/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6433 - val_loss: 2.3211\n",
            "Epoch 2569/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.7024 - val_loss: 2.2995\n",
            "Epoch 2570/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6901 - val_loss: 2.3842\n",
            "Epoch 2571/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6892 - val_loss: 2.2833\n",
            "Epoch 2572/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6270 - val_loss: 2.3078\n",
            "Epoch 2573/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6306 - val_loss: 2.2939\n",
            "Epoch 2574/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6602 - val_loss: 2.3683\n",
            "Epoch 2575/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6502 - val_loss: 2.3839\n",
            "Epoch 2576/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6427 - val_loss: 2.3090\n",
            "Epoch 2577/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6523 - val_loss: 2.3165\n",
            "Epoch 2578/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6900 - val_loss: 2.3432\n",
            "Epoch 2579/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6670 - val_loss: 2.3445\n",
            "Epoch 2580/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6490 - val_loss: 2.3013\n",
            "Epoch 2581/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6185 - val_loss: 2.3014\n",
            "Epoch 2582/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6479 - val_loss: 2.3435\n",
            "Epoch 2583/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6665 - val_loss: 2.4898\n",
            "Epoch 2584/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7014 - val_loss: 2.3131\n",
            "Epoch 2585/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6307 - val_loss: 2.3222\n",
            "Epoch 2586/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6718 - val_loss: 2.3411\n",
            "Epoch 2587/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6298 - val_loss: 2.3115\n",
            "Epoch 2588/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6388 - val_loss: 2.3335\n",
            "Epoch 2589/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6465 - val_loss: 2.3344\n",
            "Epoch 2590/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6467 - val_loss: 2.2863\n",
            "Epoch 2591/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6812 - val_loss: 2.3571\n",
            "Epoch 2592/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6599 - val_loss: 2.2935\n",
            "Epoch 2593/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6059 - val_loss: 2.4098\n",
            "Epoch 2594/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6727 - val_loss: 2.3821\n",
            "Epoch 2595/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6422 - val_loss: 2.3324\n",
            "Epoch 2596/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6186 - val_loss: 2.3379\n",
            "Epoch 2597/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6628 - val_loss: 2.4306\n",
            "Epoch 2598/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6485 - val_loss: 2.3393\n",
            "Epoch 2599/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6916 - val_loss: 2.3667\n",
            "Epoch 2600/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6692 - val_loss: 2.3222\n",
            "Epoch 2601/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6393 - val_loss: 2.2911\n",
            "Epoch 2602/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6112 - val_loss: 2.2898\n",
            "Epoch 2603/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6439 - val_loss: 2.3324\n",
            "Epoch 2604/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6534 - val_loss: 2.3208\n",
            "Epoch 2605/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6487 - val_loss: 2.3191\n",
            "Epoch 2606/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6801 - val_loss: 2.3490\n",
            "Epoch 2607/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6393 - val_loss: 2.3014\n",
            "Epoch 2608/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6288 - val_loss: 2.2892\n",
            "Epoch 2609/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6538 - val_loss: 2.3231\n",
            "Epoch 2610/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6283 - val_loss: 2.5607\n",
            "Epoch 2611/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6461 - val_loss: 2.2875\n",
            "Epoch 2612/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6714 - val_loss: 2.3381\n",
            "Epoch 2613/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6635 - val_loss: 2.2976\n",
            "Epoch 2614/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6313 - val_loss: 2.3297\n",
            "Epoch 2615/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6793 - val_loss: 2.3624\n",
            "Epoch 2616/3000\n",
            "98/98 [==============================] - 1s 11ms/step - loss: 1.6452 - val_loss: 2.2982\n",
            "Epoch 2617/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6357 - val_loss: 2.3262\n",
            "Epoch 2618/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6350 - val_loss: 2.3123\n",
            "Epoch 2619/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6676 - val_loss: 2.3103\n",
            "Epoch 2620/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6450 - val_loss: 2.3622\n",
            "Epoch 2621/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6364 - val_loss: 2.3282\n",
            "Epoch 2622/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6372 - val_loss: 2.3254\n",
            "Epoch 2623/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6665 - val_loss: 2.3751\n",
            "Epoch 2624/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6449 - val_loss: 2.3493\n",
            "Epoch 2625/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6207 - val_loss: 2.2935\n",
            "Epoch 2626/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6306 - val_loss: 2.3080\n",
            "Epoch 2627/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6658 - val_loss: 2.4045\n",
            "Epoch 2628/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6401 - val_loss: 2.3522\n",
            "Epoch 2629/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6160 - val_loss: 2.3093\n",
            "Epoch 2630/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6408 - val_loss: 2.2752\n",
            "Epoch 2631/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6499 - val_loss: 2.3231\n",
            "Epoch 2632/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6601 - val_loss: 2.3042\n",
            "Epoch 2633/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6013 - val_loss: 2.2883\n",
            "Epoch 2634/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6543 - val_loss: 2.3347\n",
            "Epoch 2635/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6609 - val_loss: 2.3376\n",
            "Epoch 2636/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6608 - val_loss: 2.3147\n",
            "Epoch 2637/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6255 - val_loss: 2.3295\n",
            "Epoch 2638/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6275 - val_loss: 2.2984\n",
            "Epoch 2639/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6649 - val_loss: 2.4154\n",
            "Epoch 2640/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6919 - val_loss: 2.3035\n",
            "Epoch 2641/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6610 - val_loss: 2.3763\n",
            "Epoch 2642/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6590 - val_loss: 2.3853\n",
            "Epoch 2643/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6982 - val_loss: 2.3548\n",
            "Epoch 2644/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6260 - val_loss: 2.4148\n",
            "Epoch 2645/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6425 - val_loss: 2.2861\n",
            "Epoch 2646/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6154 - val_loss: 2.3000\n",
            "Epoch 2647/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6784 - val_loss: 2.3892\n",
            "Epoch 2648/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6802 - val_loss: 2.2834\n",
            "Epoch 2649/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6556 - val_loss: 2.2854\n",
            "Epoch 2650/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6207 - val_loss: 2.3316\n",
            "Epoch 2651/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6577 - val_loss: 2.3052\n",
            "Epoch 2652/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6505 - val_loss: 2.4056\n",
            "Epoch 2653/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6487 - val_loss: 2.3194\n",
            "Epoch 2654/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6311 - val_loss: 2.3025\n",
            "Epoch 2655/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6405 - val_loss: 2.3023\n",
            "Epoch 2656/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6081 - val_loss: 2.3514\n",
            "Epoch 2657/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7045 - val_loss: 2.4209\n",
            "Epoch 2658/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6206 - val_loss: 2.3620\n",
            "Epoch 2659/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6569 - val_loss: 2.3559\n",
            "Epoch 2660/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6350 - val_loss: 2.2745\n",
            "Epoch 2661/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6758 - val_loss: 2.3058\n",
            "Epoch 2662/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6245 - val_loss: 2.3221\n",
            "Epoch 2663/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.7037 - val_loss: 2.3193\n",
            "Epoch 2664/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6447 - val_loss: 2.3291\n",
            "Epoch 2665/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6195 - val_loss: 2.3567\n",
            "Epoch 2666/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6979 - val_loss: 2.3941\n",
            "Epoch 2667/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6429 - val_loss: 2.2885\n",
            "Epoch 2668/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6277 - val_loss: 2.3177\n",
            "Epoch 2669/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.7027 - val_loss: 2.3604\n",
            "Epoch 2670/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6251 - val_loss: 2.3692\n",
            "Epoch 2671/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6283 - val_loss: 2.3058\n",
            "Epoch 2672/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6506 - val_loss: 2.2809\n",
            "Epoch 2673/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6214 - val_loss: 2.3163\n",
            "Epoch 2674/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6268 - val_loss: 2.3165\n",
            "Epoch 2675/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6016 - val_loss: 2.2921\n",
            "Epoch 2676/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6487 - val_loss: 2.4245\n",
            "Epoch 2677/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6455 - val_loss: 2.3460\n",
            "Epoch 2678/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6188 - val_loss: 2.3867\n",
            "Epoch 2679/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6132 - val_loss: 2.2873\n",
            "Epoch 2680/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6370 - val_loss: 2.3471\n",
            "Epoch 2681/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6136 - val_loss: 2.2887\n",
            "Epoch 2682/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6265 - val_loss: 2.3016\n",
            "Epoch 2683/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6228 - val_loss: 2.3247\n",
            "Epoch 2684/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6473 - val_loss: 2.4155\n",
            "Epoch 2685/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6441 - val_loss: 2.3537\n",
            "Epoch 2686/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6268 - val_loss: 2.2936\n",
            "Epoch 2687/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6292 - val_loss: 2.3815\n",
            "Epoch 2688/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6287 - val_loss: 2.3533\n",
            "Epoch 2689/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6433 - val_loss: 2.2994\n",
            "Epoch 2690/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6225 - val_loss: 2.2722\n",
            "Epoch 2691/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6276 - val_loss: 2.3397\n",
            "Epoch 2692/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6358 - val_loss: 2.4564\n",
            "Epoch 2693/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6483 - val_loss: 2.3072\n",
            "Epoch 2694/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6273 - val_loss: 2.2995\n",
            "Epoch 2695/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6646 - val_loss: 2.3807\n",
            "Epoch 2696/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.6146 - val_loss: 2.3101\n",
            "Epoch 2697/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6334 - val_loss: 2.3033\n",
            "Epoch 2698/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6469 - val_loss: 2.4829\n",
            "Epoch 2699/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6413 - val_loss: 2.3263\n",
            "Epoch 2700/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6365 - val_loss: 2.2862\n",
            "Epoch 2701/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6109 - val_loss: 2.2922\n",
            "Epoch 2702/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6272 - val_loss: 2.2959\n",
            "Epoch 2703/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6163 - val_loss: 2.2775\n",
            "Epoch 2704/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6692 - val_loss: 2.3031\n",
            "Epoch 2705/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6458 - val_loss: 2.3620\n",
            "Epoch 2706/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6564 - val_loss: 2.3657\n",
            "Epoch 2707/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6418 - val_loss: 2.3471\n",
            "Epoch 2708/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6475 - val_loss: 2.3029\n",
            "Epoch 2709/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6338 - val_loss: 2.2985\n",
            "Epoch 2710/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6425 - val_loss: 2.2744\n",
            "Epoch 2711/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6052 - val_loss: 2.2973\n",
            "Epoch 2712/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6090 - val_loss: 2.3253\n",
            "Epoch 2713/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6083 - val_loss: 2.2972\n",
            "Epoch 2714/3000\n",
            "98/98 [==============================] - 2s 20ms/step - loss: 1.6105 - val_loss: 2.2852\n",
            "Epoch 2715/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6380 - val_loss: 2.2953\n",
            "Epoch 2716/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6461 - val_loss: 2.3808\n",
            "Epoch 2717/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6484 - val_loss: 2.3213\n",
            "Epoch 2718/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6436 - val_loss: 2.3426\n",
            "Epoch 2719/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6169 - val_loss: 2.3474\n",
            "Epoch 2720/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6728 - val_loss: 2.2961\n",
            "Epoch 2721/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6003 - val_loss: 2.3395\n",
            "Epoch 2722/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6496 - val_loss: 2.3527\n",
            "Epoch 2723/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6118 - val_loss: 2.2947\n",
            "Epoch 2724/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6100 - val_loss: 2.3308\n",
            "Epoch 2725/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6232 - val_loss: 2.3915\n",
            "Epoch 2726/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6160 - val_loss: 2.3453\n",
            "Epoch 2727/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6436 - val_loss: 2.3214\n",
            "Epoch 2728/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6936 - val_loss: 2.3338\n",
            "Epoch 2729/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6014 - val_loss: 2.3211\n",
            "Epoch 2730/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6031 - val_loss: 2.3237\n",
            "Epoch 2731/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6269 - val_loss: 2.3239\n",
            "Epoch 2732/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6451 - val_loss: 2.3742\n",
            "Epoch 2733/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6281 - val_loss: 2.3124\n",
            "Epoch 2734/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6296 - val_loss: 2.3201\n",
            "Epoch 2735/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6146 - val_loss: 2.3209\n",
            "Epoch 2736/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5873 - val_loss: 2.3099\n",
            "Epoch 2737/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6251 - val_loss: 2.3920\n",
            "Epoch 2738/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6132 - val_loss: 2.3368\n",
            "Epoch 2739/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6400 - val_loss: 2.4138\n",
            "Epoch 2740/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6270 - val_loss: 2.3605\n",
            "Epoch 2741/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6534 - val_loss: 2.4381\n",
            "Epoch 2742/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5967 - val_loss: 2.3222\n",
            "Epoch 2743/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6275 - val_loss: 2.3068\n",
            "Epoch 2744/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6208 - val_loss: 2.3247\n",
            "Epoch 2745/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6702 - val_loss: 2.3223\n",
            "Epoch 2746/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6101 - val_loss: 2.2931\n",
            "Epoch 2747/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6287 - val_loss: 2.3236\n",
            "Epoch 2748/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5918 - val_loss: 2.3719\n",
            "Epoch 2749/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6136 - val_loss: 2.3115\n",
            "Epoch 2750/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6427 - val_loss: 2.3513\n",
            "Epoch 2751/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6036 - val_loss: 2.3880\n",
            "Epoch 2752/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6803 - val_loss: 2.3928\n",
            "Epoch 2753/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6157 - val_loss: 2.3197\n",
            "Epoch 2754/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6111 - val_loss: 2.3292\n",
            "Epoch 2755/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6331 - val_loss: 2.4167\n",
            "Epoch 2756/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6309 - val_loss: 2.6730\n",
            "Epoch 2757/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6954 - val_loss: 2.3843\n",
            "Epoch 2758/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6148 - val_loss: 2.3287\n",
            "Epoch 2759/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6486 - val_loss: 2.3725\n",
            "Epoch 2760/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6604 - val_loss: 2.3680\n",
            "Epoch 2761/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6462 - val_loss: 2.3048\n",
            "Epoch 2762/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6328 - val_loss: 2.3415\n",
            "Epoch 2763/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6037 - val_loss: 2.3978\n",
            "Epoch 2764/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6088 - val_loss: 2.3110\n",
            "Epoch 2765/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6193 - val_loss: 2.3400\n",
            "Epoch 2766/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6052 - val_loss: 2.3758\n",
            "Epoch 2767/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6156 - val_loss: 2.2977\n",
            "Epoch 2768/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6157 - val_loss: 2.3311\n",
            "Epoch 2769/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6277 - val_loss: 2.3044\n",
            "Epoch 2770/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6179 - val_loss: 2.3297\n",
            "Epoch 2771/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6625 - val_loss: 2.2868\n",
            "Epoch 2772/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6344 - val_loss: 2.3182\n",
            "Epoch 2773/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6312 - val_loss: 2.3359\n",
            "Epoch 2774/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6247 - val_loss: 2.3198\n",
            "Epoch 2775/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6075 - val_loss: 2.3283\n",
            "Epoch 2776/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5993 - val_loss: 2.3696\n",
            "Epoch 2777/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6105 - val_loss: 2.3356\n",
            "Epoch 2778/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6149 - val_loss: 2.3673\n",
            "Epoch 2779/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6346 - val_loss: 2.3225\n",
            "Epoch 2780/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6593 - val_loss: 2.3252\n",
            "Epoch 2781/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6220 - val_loss: 2.3108\n",
            "Epoch 2782/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6108 - val_loss: 2.3429\n",
            "Epoch 2783/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6011 - val_loss: 2.4373\n",
            "Epoch 2784/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6329 - val_loss: 2.3059\n",
            "Epoch 2785/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6130 - val_loss: 2.3338\n",
            "Epoch 2786/3000\n",
            "98/98 [==============================] - 1s 12ms/step - loss: 1.6477 - val_loss: 2.3554\n",
            "Epoch 2787/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6326 - val_loss: 2.3419\n",
            "Epoch 2788/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6167 - val_loss: 2.2902\n",
            "Epoch 2789/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6048 - val_loss: 2.3516\n",
            "Epoch 2790/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6018 - val_loss: 2.3052\n",
            "Epoch 2791/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6104 - val_loss: 2.4006\n",
            "Epoch 2792/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6284 - val_loss: 2.3804\n",
            "Epoch 2793/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5900 - val_loss: 2.3387\n",
            "Epoch 2794/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6131 - val_loss: 2.3419\n",
            "Epoch 2795/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6143 - val_loss: 2.3019\n",
            "Epoch 2796/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6231 - val_loss: 2.3593\n",
            "Epoch 2797/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6452 - val_loss: 2.3525\n",
            "Epoch 2798/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6147 - val_loss: 2.3189\n",
            "Epoch 2799/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5946 - val_loss: 2.3252\n",
            "Epoch 2800/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6160 - val_loss: 2.2814\n",
            "Epoch 2801/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6033 - val_loss: 2.3731\n",
            "Epoch 2802/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6356 - val_loss: 2.3679\n",
            "Epoch 2803/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6148 - val_loss: 2.4320\n",
            "Epoch 2804/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6284 - val_loss: 2.3247\n",
            "Epoch 2805/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6412 - val_loss: 2.3154\n",
            "Epoch 2806/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6419 - val_loss: 2.3267\n",
            "Epoch 2807/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6297 - val_loss: 2.3148\n",
            "Epoch 2808/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5923 - val_loss: 2.3100\n",
            "Epoch 2809/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5987 - val_loss: 2.3472\n",
            "Epoch 2810/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6118 - val_loss: 2.3393\n",
            "Epoch 2811/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5935 - val_loss: 2.3373\n",
            "Epoch 2812/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6290 - val_loss: 2.3231\n",
            "Epoch 2813/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6092 - val_loss: 2.3754\n",
            "Epoch 2814/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5999 - val_loss: 2.3284\n",
            "Epoch 2815/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6371 - val_loss: 2.3794\n",
            "Epoch 2816/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6267 - val_loss: 2.3754\n",
            "Epoch 2817/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5852 - val_loss: 2.3147\n",
            "Epoch 2818/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6278 - val_loss: 2.3823\n",
            "Epoch 2819/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6476 - val_loss: 2.3492\n",
            "Epoch 2820/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6160 - val_loss: 2.3256\n",
            "Epoch 2821/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6047 - val_loss: 2.2875\n",
            "Epoch 2822/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5831 - val_loss: 2.3062\n",
            "Epoch 2823/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6050 - val_loss: 2.3063\n",
            "Epoch 2824/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5915 - val_loss: 2.3513\n",
            "Epoch 2825/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6270 - val_loss: 2.3778\n",
            "Epoch 2826/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6409 - val_loss: 2.4136\n",
            "Epoch 2827/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6070 - val_loss: 2.3240\n",
            "Epoch 2828/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.5872 - val_loss: 2.3180\n",
            "Epoch 2829/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6007 - val_loss: 2.2871\n",
            "Epoch 2830/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6246 - val_loss: 2.3931\n",
            "Epoch 2831/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6432 - val_loss: 2.2942\n",
            "Epoch 2832/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6125 - val_loss: 2.3518\n",
            "Epoch 2833/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6364 - val_loss: 2.3496\n",
            "Epoch 2834/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6050 - val_loss: 2.3466\n",
            "Epoch 2835/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6110 - val_loss: 2.3392\n",
            "Epoch 2836/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6195 - val_loss: 2.2898\n",
            "Epoch 2837/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6181 - val_loss: 2.3241\n",
            "Epoch 2838/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.6070 - val_loss: 2.3447\n",
            "Epoch 2839/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5915 - val_loss: 2.2851\n",
            "Epoch 2840/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6223 - val_loss: 2.3236\n",
            "Epoch 2841/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6240 - val_loss: 2.3356\n",
            "Epoch 2842/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6226 - val_loss: 2.3098\n",
            "Epoch 2843/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6184 - val_loss: 2.3502\n",
            "Epoch 2844/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6893 - val_loss: 2.5275\n",
            "Epoch 2845/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6352 - val_loss: 2.3054\n",
            "Epoch 2846/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5931 - val_loss: 2.3424\n",
            "Epoch 2847/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6394 - val_loss: 2.3000\n",
            "Epoch 2848/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5661 - val_loss: 2.3889\n",
            "Epoch 2849/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6149 - val_loss: 2.3276\n",
            "Epoch 2850/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5727 - val_loss: 2.4492\n",
            "Epoch 2851/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6345 - val_loss: 2.3231\n",
            "Epoch 2852/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5535 - val_loss: 2.3074\n",
            "Epoch 2853/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6200 - val_loss: 2.3261\n",
            "Epoch 2854/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6190 - val_loss: 2.3116\n",
            "Epoch 2855/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6169 - val_loss: 2.3131\n",
            "Epoch 2856/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6171 - val_loss: 2.3274\n",
            "Epoch 2857/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6018 - val_loss: 2.4421\n",
            "Epoch 2858/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5854 - val_loss: 2.3058\n",
            "Epoch 2859/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6086 - val_loss: 2.3996\n",
            "Epoch 2860/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6539 - val_loss: 2.2968\n",
            "Epoch 2861/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6246 - val_loss: 2.3197\n",
            "Epoch 2862/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6006 - val_loss: 2.3709\n",
            "Epoch 2863/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6519 - val_loss: 2.3644\n",
            "Epoch 2864/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6039 - val_loss: 2.2956\n",
            "Epoch 2865/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6195 - val_loss: 2.3074\n",
            "Epoch 2866/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6146 - val_loss: 2.3295\n",
            "Epoch 2867/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6118 - val_loss: 2.3367\n",
            "Epoch 2868/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5937 - val_loss: 2.3258\n",
            "Epoch 2869/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5775 - val_loss: 2.3537\n",
            "Epoch 2870/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6214 - val_loss: 2.2949\n",
            "Epoch 2871/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5999 - val_loss: 2.3651\n",
            "Epoch 2872/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5803 - val_loss: 2.3127\n",
            "Epoch 2873/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6568 - val_loss: 2.3102\n",
            "Epoch 2874/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5901 - val_loss: 2.4239\n",
            "Epoch 2875/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6306 - val_loss: 2.3219\n",
            "Epoch 2876/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6164 - val_loss: 2.3216\n",
            "Epoch 2877/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5911 - val_loss: 2.3287\n",
            "Epoch 2878/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6339 - val_loss: 2.3399\n",
            "Epoch 2879/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5948 - val_loss: 2.3018\n",
            "Epoch 2880/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5910 - val_loss: 2.3169\n",
            "Epoch 2881/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5965 - val_loss: 2.2839\n",
            "Epoch 2882/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6047 - val_loss: 2.3879\n",
            "Epoch 2883/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6419 - val_loss: 2.6170\n",
            "Epoch 2884/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6436 - val_loss: 2.3846\n",
            "Epoch 2885/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6246 - val_loss: 2.3090\n",
            "Epoch 2886/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6195 - val_loss: 2.3071\n",
            "Epoch 2887/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.6133 - val_loss: 2.3101\n",
            "Epoch 2888/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.5765 - val_loss: 2.3254\n",
            "Epoch 2889/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5971 - val_loss: 2.2916\n",
            "Epoch 2890/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5845 - val_loss: 2.3583\n",
            "Epoch 2891/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6167 - val_loss: 2.3471\n",
            "Epoch 2892/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6048 - val_loss: 2.3006\n",
            "Epoch 2893/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6103 - val_loss: 2.3327\n",
            "Epoch 2894/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5696 - val_loss: 2.2919\n",
            "Epoch 2895/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5904 - val_loss: 2.3264\n",
            "Epoch 2896/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6316 - val_loss: 2.3417\n",
            "Epoch 2897/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5849 - val_loss: 2.3863\n",
            "Epoch 2898/3000\n",
            "98/98 [==============================] - 2s 20ms/step - loss: 1.6381 - val_loss: 2.2981\n",
            "Epoch 2899/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.6176 - val_loss: 2.3543\n",
            "Epoch 2900/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6009 - val_loss: 2.3208\n",
            "Epoch 2901/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5938 - val_loss: 2.3696\n",
            "Epoch 2902/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5756 - val_loss: 2.3192\n",
            "Epoch 2903/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6603 - val_loss: 2.3027\n",
            "Epoch 2904/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5798 - val_loss: 2.3365\n",
            "Epoch 2905/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6193 - val_loss: 2.3646\n",
            "Epoch 2906/3000\n",
            "98/98 [==============================] - 2s 19ms/step - loss: 1.6097 - val_loss: 2.3210\n",
            "Epoch 2907/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5794 - val_loss: 2.3143\n",
            "Epoch 2908/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6165 - val_loss: 2.3486\n",
            "Epoch 2909/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6420 - val_loss: 2.3343\n",
            "Epoch 2910/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5898 - val_loss: 2.3034\n",
            "Epoch 2911/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5993 - val_loss: 2.4163\n",
            "Epoch 2912/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.5974 - val_loss: 2.3503\n",
            "Epoch 2913/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6210 - val_loss: 2.3148\n",
            "Epoch 2914/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.5741 - val_loss: 2.3233\n",
            "Epoch 2915/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6127 - val_loss: 2.3308\n",
            "Epoch 2916/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5924 - val_loss: 2.3708\n",
            "Epoch 2917/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5797 - val_loss: 2.3076\n",
            "Epoch 2918/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6195 - val_loss: 2.3048\n",
            "Epoch 2919/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5895 - val_loss: 2.3219\n",
            "Epoch 2920/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5810 - val_loss: 2.3053\n",
            "Epoch 2921/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5885 - val_loss: 2.3546\n",
            "Epoch 2922/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5831 - val_loss: 2.3284\n",
            "Epoch 2923/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6201 - val_loss: 2.3157\n",
            "Epoch 2924/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5823 - val_loss: 2.3313\n",
            "Epoch 2925/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6076 - val_loss: 2.3506\n",
            "Epoch 2926/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5960 - val_loss: 2.3615\n",
            "Epoch 2927/3000\n",
            "98/98 [==============================] - 2s 18ms/step - loss: 1.5920 - val_loss: 2.3467\n",
            "Epoch 2928/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5676 - val_loss: 2.4169\n",
            "Epoch 2929/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6101 - val_loss: 2.3733\n",
            "Epoch 2930/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5936 - val_loss: 2.2949\n",
            "Epoch 2931/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6010 - val_loss: 2.3609\n",
            "Epoch 2932/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6164 - val_loss: 2.4316\n",
            "Epoch 2933/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6405 - val_loss: 2.3393\n",
            "Epoch 2934/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6309 - val_loss: 2.3941\n",
            "Epoch 2935/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6348 - val_loss: 2.3239\n",
            "Epoch 2936/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6076 - val_loss: 2.3298\n",
            "Epoch 2937/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5840 - val_loss: 2.4539\n",
            "Epoch 2938/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.6118 - val_loss: 2.3179\n",
            "Epoch 2939/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5875 - val_loss: 2.3355\n",
            "Epoch 2940/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5825 - val_loss: 2.3212\n",
            "Epoch 2941/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.5513 - val_loss: 2.3101\n",
            "Epoch 2942/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5796 - val_loss: 2.3278\n",
            "Epoch 2943/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5842 - val_loss: 2.4170\n",
            "Epoch 2944/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6204 - val_loss: 2.2991\n",
            "Epoch 2945/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5682 - val_loss: 2.3224\n",
            "Epoch 2946/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.5857 - val_loss: 2.3679\n",
            "Epoch 2947/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.6145 - val_loss: 2.3191\n",
            "Epoch 2948/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6099 - val_loss: 2.4130\n",
            "Epoch 2949/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6380 - val_loss: 2.3643\n",
            "Epoch 2950/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6170 - val_loss: 2.5334\n",
            "Epoch 2951/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6667 - val_loss: 2.3437\n",
            "Epoch 2952/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6282 - val_loss: 2.2980\n",
            "Epoch 2953/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5787 - val_loss: 2.3337\n",
            "Epoch 2954/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5692 - val_loss: 2.2923\n",
            "Epoch 2955/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5895 - val_loss: 2.3019\n",
            "Epoch 2956/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5792 - val_loss: 2.3624\n",
            "Epoch 2957/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5639 - val_loss: 2.3612\n",
            "Epoch 2958/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6171 - val_loss: 2.3440\n",
            "Epoch 2959/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5886 - val_loss: 2.3893\n",
            "Epoch 2960/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5893 - val_loss: 2.3418\n",
            "Epoch 2961/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5870 - val_loss: 2.2988\n",
            "Epoch 2962/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6077 - val_loss: 2.3640\n",
            "Epoch 2963/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6048 - val_loss: 2.3123\n",
            "Epoch 2964/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5901 - val_loss: 2.3343\n",
            "Epoch 2965/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5605 - val_loss: 2.3154\n",
            "Epoch 2966/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5963 - val_loss: 2.3063\n",
            "Epoch 2967/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5934 - val_loss: 2.2961\n",
            "Epoch 2968/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6150 - val_loss: 2.3118\n",
            "Epoch 2969/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.5824 - val_loss: 2.3187\n",
            "Epoch 2970/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6333 - val_loss: 2.3640\n",
            "Epoch 2971/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5816 - val_loss: 2.3059\n",
            "Epoch 2972/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5767 - val_loss: 2.3620\n",
            "Epoch 2973/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.5729 - val_loss: 2.3153\n",
            "Epoch 2974/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6476 - val_loss: 2.3364\n",
            "Epoch 2975/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5826 - val_loss: 2.4724\n",
            "Epoch 2976/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5950 - val_loss: 2.3507\n",
            "Epoch 2977/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5883 - val_loss: 2.3572\n",
            "Epoch 2978/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5664 - val_loss: 2.2881\n",
            "Epoch 2979/3000\n",
            "98/98 [==============================] - 2s 17ms/step - loss: 1.5868 - val_loss: 2.3207\n",
            "Epoch 2980/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5904 - val_loss: 2.3606\n",
            "Epoch 2981/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5843 - val_loss: 2.3647\n",
            "Epoch 2982/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5790 - val_loss: 2.3367\n",
            "Epoch 2983/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5757 - val_loss: 2.3502\n",
            "Epoch 2984/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5810 - val_loss: 2.3163\n",
            "Epoch 2985/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6308 - val_loss: 2.3499\n",
            "Epoch 2986/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6339 - val_loss: 2.3728\n",
            "Epoch 2987/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.6010 - val_loss: 2.3492\n",
            "Epoch 2988/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5869 - val_loss: 2.3922\n",
            "Epoch 2989/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5533 - val_loss: 2.4023\n",
            "Epoch 2990/3000\n",
            "98/98 [==============================] - 2s 15ms/step - loss: 1.5976 - val_loss: 2.3577\n",
            "Epoch 2991/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.6151 - val_loss: 2.3153\n",
            "Epoch 2992/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5554 - val_loss: 2.4174\n",
            "Epoch 2993/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.6559 - val_loss: 2.3287\n",
            "Epoch 2994/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5713 - val_loss: 2.3261\n",
            "Epoch 2995/3000\n",
            "98/98 [==============================] - 1s 13ms/step - loss: 1.5854 - val_loss: 2.3028\n",
            "Epoch 2996/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5706 - val_loss: 2.3495\n",
            "Epoch 2997/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5990 - val_loss: 2.3372\n",
            "Epoch 2998/3000\n",
            "98/98 [==============================] - 1s 14ms/step - loss: 1.5684 - val_loss: 2.3296\n",
            "Epoch 2999/3000\n",
            "98/98 [==============================] - 1s 15ms/step - loss: 1.5731 - val_loss: 2.3201\n",
            "Epoch 3000/3000\n",
            "98/98 [==============================] - 2s 16ms/step - loss: 1.6024 - val_loss: 2.3043\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# Début du chrono\n",
        "start_time_mean_state = time.time()\n",
        "# Enregistrer les temps de départ\n",
        "step_times = []\n",
        "\n",
        "# Définir un callback pour mesurer les temps de chaque étape\n",
        "class TimingCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_begin(self, batch, logs=None):\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        step_time = time.time() - self.start_time\n",
        "        step_times.append(step_time)\n",
        "# Train the model\n",
        "history_mean_zone=model.fit(X_train, y_train, epochs=3000, batch_size=250, validation_data=(X_test, y_test),callbacks=[TimingCallback()])\n",
        "# Fin du chrono\n",
        "end_time_mean_state = time.time()"
      ],
      "id": "f4NLfpuXkMtL"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSou9AMYkQB6",
        "outputId": "ec2938d3-32dd-4da0-a9ba-31d39e1b7492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le temps d'entraînement est de 3711.664652109146 secondes\n",
            "191/191 [==============================] - 1s 6ms/step - loss: 2.3043\n"
          ]
        }
      ],
      "source": [
        "# Calcul de la durée d'entraînement en secondes\n",
        "training_time_mean_state = end_time_mean_state - start_time_mean_state\n",
        "# Affichage de la durée d'entraînement\n",
        "print(\"Le temps d'entraînement est de\", training_time_mean_state, \"secondes\")\n",
        "# Evaluate the model\n",
        "loss_mean_state = model.evaluate(X_test, y_test)"
      ],
      "id": "qSou9AMYkQB6"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhMXfb4UkdlH",
        "outputId": "c791f366-55fd-4e26-9d03-6f6b11532b9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 594ms/step\n",
            "Predicted WQI: 756.3591\n"
          ]
        }
      ],
      "source": [
        "# Predict the WQI for a new sample\n",
        "new_sample = np.array([[7.5, 8.0, 300, 2.0, 0.5, 100, 2005,20,30]])\n",
        "new_sample = scaler.transform(new_sample)\n",
        "new_sample = new_sample.reshape((new_sample.shape[0], new_sample.shape[1], 1))\n",
        "prediction = model.predict(new_sample)\n",
        "print('Predicted WQI:', prediction[0][0])"
      ],
      "id": "uhMXfb4UkdlH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqzWDYDDE1XL"
      },
      "source": [
        "*comparaison*"
      ],
      "id": "YqzWDYDDE1XL"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "-hkZqOE2khVu",
        "outputId": "7e048b95-2241-4ef9-f359-57f9b43d0290"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Afficher les graphes d'accuracy et loss selon l'epoch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "plt.plot(history_mean_zone.history['loss'], label='Mean squared error')\n",
        "plt.plot(history_mean_zone.history['val_loss'], label='val Mean squared error')\n",
        "plt.title('Mean squared error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "# agrandir la taille de la figure\n",
        "fig.set_size_inches(16, 14)\n",
        "plt.show()"
      ],
      "id": "-hkZqOE2khVu"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Zw5K1O9RR1kY"
      },
      "outputs": [],
      "source": [
        "model.save('prediction_wqi_par_cnn_hong_kong_v1.h5')"
      ],
      "id": "Zw5K1O9RR1kY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WDP6XC4gtee"
      },
      "id": "8WDP6XC4gtee",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}