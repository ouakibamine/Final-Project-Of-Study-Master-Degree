{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 77
    },
    "id": "awNw4_TffvRd",
    "outputId": "5f377082-d7fb-4cf0-b08d-632296b9e8e9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-33839c18-4a55-4350-a1e8-2e75aca452ad\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-33839c18-4a55-4350-a1e8-2e75aca452ad\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Water_dataX_mean_state_hong_kong.csv to Water_dataX_mean_state_hong_kong (2).csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense ,Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "from google.colab import files\n",
    "uploaded= files.upload()\n",
    "\n",
    "# Load the data into a pandas DataFrame called df\n",
    "df=pd.read_csv('Water_dataX_mean_state_hong_kong.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3lXRGWYErpU"
   },
   "outputs": [],
   "source": [
    "#df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aQGwARyfrzXe",
    "outputId": "ad220476-ed57-4bd5-c5e1-65a97b2779e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30528, 2)\n",
      "data_sort  = \n",
      "            dates    wqi\n",
      "0       4/29/1986   62.0\n",
      "1       5/19/1986   64.0\n",
      "2       6/18/1986   67.0\n",
      "3       7/24/1986   68.0\n",
      "4       8/15/1986   68.0\n",
      "...           ...    ...\n",
      "30523   7/15/2020   93.0\n",
      "30524   9/16/2020   98.0\n",
      "30525   10/7/2020  100.0\n",
      "30526  11/16/2020   96.0\n",
      "30527   12/2/2020   89.0\n",
      "\n",
      "[30528 rows x 2 columns]\n",
      "X_train = \n",
      "[[0.57777778 0.6        0.63333333 ... 0.86666667 0.96666667 0.85555556]\n",
      " [0.6        0.63333333 0.64444444 ... 0.96666667 0.85555556 1.        ]\n",
      " [0.63333333 0.64444444 0.64444444 ... 0.85555556 1.         0.85555556]\n",
      " ...\n",
      " [0.28888889 0.26666667 0.26666667 ... 0.58888889 0.56666667 0.54444444]\n",
      " [0.26666667 0.26666667 0.26666667 ... 0.56666667 0.54444444 0.67777778]\n",
      " [0.26666667 0.26666667 0.21111111 ... 0.54444444 0.67777778 0.73333333]]\n"
     ]
    }
   ],
   "source": [
    "#df.head(1)\n",
    "\n",
    "def data_zone_river_station (df) : \n",
    "  df=df[['water control zone','river','station','dates','wqi']]\n",
    "  data = pd.DataFrame()\n",
    "\n",
    "  B = set(df['water control zone'])\n",
    "  #begin = 30000\n",
    "  #end = 30528\n",
    "  for zone in B : \n",
    "    C = set(df[df['water control zone']==zone]['river'])\n",
    "    for river in C : \n",
    "      D=set(df[df['river']==river]['station'])\n",
    "      for station in D :\n",
    "        subdata = df[['dates', 'wqi']][(df['water control zone'] ==zone) & (df['river'] == river) & (df['station'] == station)]\n",
    "        data = pd.concat([data, subdata])\n",
    "\n",
    "  print(data.shape)\n",
    "  data_sort = data.sort_index()\n",
    "  print(\"data_sort  = \" )\n",
    "  print(data_sort)\n",
    "  #plt.plot(data_sort.index[begin : end], data_sort['wqi'][begin:end], label='Observed')\n",
    "  #plt.show()\n",
    "\n",
    "  data_sort.to_csv('data1.csv', index=False)\n",
    "\n",
    "\n",
    "  # Load the data into a pandas DataFrame called df\n",
    "  df_selected = pd.read_csv('data1.csv', index_col=0, parse_dates=True)\n",
    "\n",
    "  return df_selected\n",
    "\n",
    "def choice_of_zone_river_station (df) : \n",
    "  df=df[['water control zone','river','station','dates','wqi']]\n",
    "\n",
    "  B = set(df['water control zone'])\n",
    "  print('choisir votre zone parmi : ')    \n",
    "  print(B)\n",
    "  zone=input()\n",
    "\n",
    "  C = set(df[df['water control zone']==zone]['river'])\n",
    "  print('choisir votre rivier parmi ')\n",
    "  print(C)\n",
    "  river=input()\n",
    "\n",
    "  D=set(df[df['river']==river]['station'])\n",
    "  print('choisis votre station parmi :')\n",
    "  print(D)\n",
    "  station=input()\n",
    "\n",
    "  data = df[['dates', 'wqi']][(df['water control zone'] ==zone) & (df['river'] == river) & (df['station'] == station)]\n",
    "  data.to_csv('data1.csv', index=False)\n",
    "  \n",
    "  # Load the data into a pandas DataFrame called df\n",
    "  df_selected = pd.read_csv('data1.csv', index_col=0, parse_dates=True)\n",
    "  zone_river_station = {\"Selected zone = \": zone, \"Selected river = \": river, \"Selected station = \": station}\n",
    "\n",
    "  return df_selected, zone_river_station\n",
    "\n",
    "def Split_into_train_test (data_selected, coef) : \n",
    "\n",
    "  # Scale the data using MinMaxScaler\n",
    "  scaler = MinMaxScaler()\n",
    "  scaled_data = scaler.fit_transform(data_selected.values.reshape(-1, 1))\n",
    "\n",
    "  # Split the data into training and testing sets\n",
    "  train_size = int(len(scaled_data) * coef)\n",
    "  train_data = scaled_data[:train_size, :]\n",
    "  test_data = scaled_data[train_size:, :]\n",
    "\n",
    "  return train_data, test_data, scaler, scaled_data\n",
    "\n",
    "# Define a function to create sequences of data \n",
    "def create_sequences(data, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(seq_length, len(data)):\n",
    "        X.append(data[i-seq_length:i, 0])\n",
    "        y.append(data[i, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Build the CNN model\n",
    "def CNN_model(X_train) :\n",
    "  model = Sequential()\n",
    "  model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "  model.add(MaxPooling1D(pool_size=1))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=1))\n",
    "  model.add(BatchNormalization())\n",
    "  model.add(Conv1D(filters=128, kernel_size=1, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=1))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(units=128, activation='relu'))\n",
    "  model.add(Dense(units=1, activation='linear'))\n",
    "  return model\n",
    "\n",
    "# Define the LSTM model\n",
    "def LSTM_model(X_train) :\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(units=150, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "  model.add(LSTM(units=50, return_sequences=True))\n",
    "  model.add(LSTM(units=50))\n",
    "  model.add(Dropout(0.1))\n",
    "  model.add(Dense(units=1))\n",
    "  return model\n",
    "\n",
    "def ARIMA_model(X_train) :\n",
    "\n",
    "  # Create an ARIMA model with order (1, 3, 2)\n",
    "  model = sm.tsa.arima.ARIMA(X_train, order=(5, 1, 0))\n",
    "  #model = sm.tsa.arima.ARIMA(data_selected['wqi'], order=(1, 1, 2))\n",
    "  \n",
    "  return model\n",
    "\n",
    "def Choice_of_model () :\n",
    "  models = {'CNN', 'LSTM', 'ARIMA'}\n",
    "  print('choisir un modèle parmi : ')    \n",
    "  print(models)\n",
    "\n",
    "  choix_model=input()\n",
    "\n",
    "  return choix_model\n",
    "\n",
    "# Définir un callback pour mesurer les temps de chaque étape\n",
    "step_times = []\n",
    "class TimingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        self.start_time = time.time()\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        step_time = time.time() - self.start_time\n",
    "        step_times.append(step_time)\n",
    "\n",
    "def run_model (choix_model, X_train, y_train, X_test, y_test, batch_size, epochs) : \n",
    "\n",
    "  # Train the model\n",
    "  # Début du chrono\n",
    "  start_time = time.time()\n",
    "  # Enregistrer les temps de départ\n",
    "\n",
    "  # Train the model\n",
    "  if (str(choix_model)== 'ARIMA') :     \n",
    "    model = ARIMA_model(X_train)\n",
    "    model = model.fit()\n",
    "    # save model\n",
    "    model.save('model.pkl')\n",
    "    history = model # a modifier\n",
    "  elif (choix_model== 'CNN') :\n",
    "    model = CNN_model(X_train)\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    history=model.fit(X_train, y_train, batch_size, epochs, validation_data=(X_test, y_test),callbacks=[TimingCallback()])\n",
    "    model.save('forcasting_hong_kong_'+str(choix_model)+'.h5')\n",
    "  elif (choix_model== 'LSTM') :\n",
    "    model = LSTM_model(X_train)\n",
    "    model.compile(optimizer='adam',loss='mse')\n",
    "    history=model.fit(X_train, y_train, batch_size, epochs, validation_data=(X_test, y_test),callbacks=[TimingCallback()])\n",
    "    model.save('forcasting_hong_kong_'+str(choix_model)+'.h5')\n",
    "  else : \n",
    "    print(\"choix erroné !! \")   \n",
    "\n",
    "  # Fin du chrono\n",
    "  end_time = time.time()\n",
    "\n",
    "  # Calcul de la durée d'entraînement en secondes\n",
    "  training_time = end_time - start_time\n",
    "\n",
    "  return model, history, training_time\n",
    "\n",
    "def prediction_on_test_data (choix_model, model) :\n",
    "  prediction =[]\n",
    "  # Make predictions on the test data\n",
    "  if (str(choix_model)== 'ARIMA') :\n",
    "    #model = ARIMAResults.load('model.pkl')\n",
    "    with open('model.pkl', 'rb') as pkl:\n",
    "      #prediction = pickle.load(pkl).predict(start=data_selected.index[seq_length])\n",
    "      prediction = pickle.load(pkl).predict(start = X_test[0], end = X_test[len(X_test)-1])\n",
    "  else :\n",
    "    x_t = []\n",
    "    for i in range(seq_length, len(scaled_data)):\n",
    "      x_t.append(scaled_data[i-seq_length:i, :])\n",
    "    x_t = np.array(x_t)\n",
    "    #prediction = model.predict(x_t)\n",
    "    prediction = model.predict(X_test)  ### a tester\n",
    "    # Rescale the data back to the original scale\n",
    "    prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "  return prediction\n",
    "\n",
    "def run_models (X_train, y_train, X_test, y_test, batch_size, epochs) :\n",
    "  predictions = dict()\n",
    "  MSE = dict()\n",
    "  CPU_time = dict()\n",
    "  histories = dict()\n",
    "  y_observed = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "  #models = {'CNN', 'ARIMA', 'LSTM'}\n",
    "  models = {'ARIMA'}\n",
    "  for choice in models :\n",
    "    if (choice == 'CNN') :\n",
    "      print ('************** '+ str(choice) + ' model is running ********* : ')\n",
    "      model, history, training_time = run_model (choice, X_train, y_train, X_test, y_test, batch_size, epochs) \n",
    "      predictions [choice] = prediction_on_test_data (choice, model)\n",
    "      # la durée d'entraînement\n",
    "      CPU_time [choice] = training_time\n",
    "      #Calculate the MSE\n",
    "      mse = mean_squared_error(y_observed,predictions[choice])\n",
    "      MSE[choice] = mse\n",
    "      histories[choice] = history \n",
    "    elif (choice == 'LSTM') :\n",
    "      print ('************** '+ str(choice) + ' model is running ********* : ')\n",
    "      model, history, training_time = run_model (choice, X_train, y_train, X_test, y_test, batch_size, epochs)\n",
    "      predictions [choice] = prediction_on_test_data (choice, model)\n",
    "      #la durée d'entraînement\n",
    "      CPU_time [choice] = training_time\n",
    "      #Calculate the MSE\n",
    "      mse = mean_squared_error(y_observed,predictions[choice])\n",
    "      MSE[choice] = mse\n",
    "      histories[choice] = history \n",
    "    elif (choice == 'ARIMA') :\n",
    "      print ('************** '+ str(choice) + ' model is running ********* : ')\n",
    "      model, history, training_time = run_model (choice, X_train, y_train, X_test, y_test, batch_size, epochs) \n",
    "      res = np.array(prediction_on_test_data (choice, model)).values\n",
    "      predictions [choice] = res \n",
    "      #la durée d'entraînement\n",
    "      CPU_time [choice] = training_time\n",
    "      #Calculate the MSE\n",
    "      mse = mean_squared_error(y_observed, predictions[choice])\n",
    "      MSE[choice] = mse\n",
    "      histories[choice] = history \n",
    "    else : \n",
    "      print(\"choix erroné !! \")\n",
    "\n",
    "  return predictions, MSE, CPU_time, histories\n",
    "\n",
    "def plot_predictions (predictions) :\n",
    "\n",
    "  fig, ax = plt.subplots(3, 1, figsize=(12, 10) , height_ratios=[2, 2, 2])\n",
    "  fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "  # Data observed \n",
    "  #y_observed = scaler.inverse_transform(scaled_data[seq_length:len(scaled_data)])\n",
    "  y_observed = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "  \n",
    "  # Tracer les valeurs observées et prédites\n",
    "  #models = {0:'LSTM', 1:'ARIMA', 2:'CNN'}\n",
    "  models = {0:'ARIMA'}\n",
    "  for i in models.keys() : \n",
    "    ax[i].plot(y_observed, label='Observed')\n",
    "    ax[i].plot(predictions[models[i]], label='Predicted')\n",
    "    ax[i].set_title('Prediction with '+str(models[i]) + ' model', fontsize=16, color='blue')\n",
    "    ax[i].set_ylabel('WQI')\n",
    "    ax[i].legend()\n",
    "\n",
    "  plt.show() \n",
    "\n",
    "############\n",
    "#data_selected, zone_river_station = choice_of_zone_river_station(df)\n",
    "data_selected = data_zone_river_station(df)\n",
    "\n",
    "train_data, test_data, scaler, scaled_data = Split_into_train_test (data_selected, 0.95)\n",
    "train_size = train_data.shape[0]\n",
    "\n",
    "# Create sequences of data\n",
    "seq_length = 30\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, seq_length)\n",
    "X_test, y_test = create_sequences(test_data, seq_length)\n",
    "\n",
    "print(\"X_train = \")\n",
    "print(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "2FNA0h3AYnj9",
    "outputId": "6997eb9a-eacd-457e-8654-c47f17e5feb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************** ARIMA model is running ********* : \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-e84a6500e9e9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model, choix_model = Choice_of_model ()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCPU_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_models\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplot_predictions\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-32a40b9db106>\u001b[0m in \u001b[0;36mrun_models\u001b[0;34m(X_train, y_train, X_test, y_test, batch_size, epochs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ARIMA'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'************** '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' model is running ********* : '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m       \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_on_test_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[0mpredictions\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-32a40b9db106>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(choix_model, X_train, y_train, X_test, y_test, batch_size, epochs)\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchoix_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m \u001b[0;34m'ARIMA'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mARIMA_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-32a40b9db106>\u001b[0m in \u001b[0;36mARIMA_model\u001b[0;34m(X_train)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0;31m# Create an ARIMA model with order (1, 3, 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marima\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mARIMA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m   \u001b[0;31m#model = sm.tsa.arima.ARIMA(data_selected['wqi'], order=(1, 1, 2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/arima/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, order, seasonal_order, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# this criteria. Instead, we'll just make sure that the parameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# estimates from those methods satisfy the criteria.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         self._spec_arima = SARIMAXSpecification(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseasonal_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseasonal_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mtrend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_stationarity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_invertibility\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/statsmodels/tsa/arima/specification.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, endog, exog, order, seasonal_order, ar_order, diff, ma_order, seasonal_ar_order, seasonal_diff, seasonal_ma_order, seasonal_periods, trend, enforce_stationarity, enforce_invertibility, concentrate_scale, trend_offset, dates, freq, missing, validate_specification)\u001b[0m\n\u001b[1;32m    452\u001b[0m         if (validate_specification and not faux_endog and\n\u001b[1;32m    453\u001b[0m                 self.endog.ndim > 1 and self.endog.shape[1] > 1):\n\u001b[0;32m--> 454\u001b[0;31m             raise ValueError('SARIMAX models require univariate `endog`. Got'\n\u001b[0m\u001b[1;32m    455\u001b[0m                              ' shape %s.' % str(self.endog.shape))\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: SARIMAX models require univariate `endog`. Got shape (28971, 30)."
     ]
    }
   ],
   "source": [
    "#model, choix_model = Choice_of_model ()\n",
    "\n",
    "predictions, MSE, CPU_time, histories = run_models (X_train, y_train,  X_test, y_test, batch_size=32,  epochs = 3)\n",
    "plot_predictions (predictions)\n",
    "\n",
    "print(\"######## Test on the selected data ###### :\" )\n",
    "#print(zone_river_station)\n",
    "\n",
    "#models = {'ARIMA', 'LSTM', 'CNN'}\n",
    "models = {'ARIMA'}\n",
    "print('*********************************')\n",
    "for choice in models :\n",
    "  print(\"CPU time of \"+ str(choice)+ \" model : \", CPU_time [choice], \"secondes\")\n",
    "print('*********************************')  \n",
    "for choice in models :\n",
    "  print('Mean Square Erreur of ' + str(choice)+ ' model:', MSE[choice])\n",
    "\n",
    "  #print('History ' + str(choice)+ ' model:', histories[choice])\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(12, 10) , height_ratios=[2, 2, 2])\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "#models = {0:'LSTM', 1:'ARIMA', 2:'CNN'}\n",
    "models = {0:'ARIMA'}\n",
    "from pandas import DataFrame\n",
    "\n",
    "for i in models.keys()  :\n",
    "  #print(histories[choice].history.keys())\n",
    "  if (i==1) :\n",
    "    ax[i].plot(DataFrame(histories['ARIMA'].resid)) ## vérifier l'équivalence entre residu d'Arima et loss de CNN et LSTN \n",
    "  else : \n",
    "    ax[i].plot(histories[models[i]].history['loss'])\n",
    "    ax[i].plot(histories[models[i]].history['val_loss'])\n",
    "    ax[i].legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "  ax[i].set_title('Loss of '+ str(models[i])+ ' model', fontsize=16, color='blue')\n",
    "  ax[i].set_ylabel('loss')\n",
    "  ax[i].set_xlabel('epoch')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
